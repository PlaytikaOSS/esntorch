{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides a use case example of the ``EsnTorch`` library.\n",
    "It described the implementation of an **Echo State Network (ESN)**\n",
    "for text classification on the **TREC-6** dataset.\n",
    "\n",
    "The instantiation, training and evaluation of an ESN for text classification\n",
    "is achieved via the following steps:\n",
    "- Import the required modules\n",
    "- Create the dataloaders\n",
    "- Instantiate the ESN by specifying:\n",
    "    - a reservoir\n",
    "    - a loss function\n",
    "    - a learning algorithm\n",
    "- Train the ESN\n",
    "- Training and testing results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers==4.8.2\n",
    "# !pip install datasets==1.7.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment this if library is installed\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath(\"..\"))\n",
    "# sys.path.insert(0, os.path.abspath(\"../..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import torch\n",
    "\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets, DatasetDict\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers.data.data_collator import DataCollatorWithPadding\n",
    "\n",
    "import esntorch.core.reservoir as res\n",
    "import esntorch.core.learning_algo as la\n",
    "import esntorch.core.pooling_strategy as ps\n",
    "import esntorch.core.esn as esn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device and Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Tokenize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom functions for loading and preparing data\n",
    "\n",
    "def tokenize(sample):\n",
    "    \"\"\"Tokenize sample\"\"\"\n",
    "    \n",
    "    sample = tokenizer(sample['text'], truncation=True, padding=False, return_length=True)\n",
    "    \n",
    "    return sample\n",
    "    \n",
    "def load_and_prepare_dataset(dataset_name, split, cache_dir):\n",
    "    \"\"\"\n",
    "    Load dataset from the datasets library of HuggingFace.\n",
    "    Tokenize and add length.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load dataset\n",
    "    dataset = load_dataset(dataset_name, split=split, cache_dir=CACHE_DIR)\n",
    "    if isinstance(dataset, list):\n",
    "        dataset = DatasetDict({\"train\": dataset[0],\"test\": dataset[1]})\n",
    "    \n",
    "    # Rename label column for tokenization purposes (use 'label-fine' for fine-grained labels)\n",
    "    dataset = dataset.rename_column('label-coarse', 'labels')\n",
    "    \n",
    "    # Tokenize data\n",
    "    dataset = dataset.map(tokenize, batched=True)\n",
    "    dataset = dataset.rename_column('length', 'lengths')\n",
    "    dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels', 'lengths'])\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset trec (cache_dir/trec/default/1.1.0/751da1ab101b8d297a3d6e9c79ee9b0173ff94c4497b75677b59b61d5467a9b9)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38e8af7bfd8e40f5ab276e59eccc6230",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c9be37c67d345109d9cf71e6bed65ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load BERT tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load and prepare data\n",
    "CACHE_DIR = 'cache_dir/' # put your path here\n",
    "\n",
    "#full_dataset = load_and_prepare_dataset('trec', split=None, 'test[:10%]'], cache_dir=CACHE_DIR)\n",
    "full_dataset = load_and_prepare_dataset('trec', split=['train[:20%]', 'test[:10%]'], cache_dir=CACHE_DIR)\n",
    "train_dataset = full_dataset['train'].sort(\"lengths\")\n",
    "test_dataset = full_dataset['test'].sort(\"lengths\")\n",
    "\n",
    "# Create dict of all datasets\n",
    "dataset_d = {\n",
    "    'train': train_dataset,\n",
    "    'test': test_dataset\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': Dataset({\n",
       "     features: ['attention_mask', 'input_ids', 'label-fine', 'labels', 'lengths', 'text', 'token_type_ids'],\n",
       "     num_rows: 1090\n",
       " }),\n",
       " 'test': Dataset({\n",
       "     features: ['attention_mask', 'input_ids', 'label-fine', 'labels', 'lengths', 'text', 'token_type_ids'],\n",
       "     num_rows: 50\n",
       " })}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dict of dataloaders\n",
    "\n",
    "dataloader_d = {}\n",
    "\n",
    "for k, v in dataset_d.items():\n",
    "    dataloader_d[k] = torch.utils.data.DataLoader(v, batch_size=256, collate_fn=DataCollatorWithPadding(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': <torch.utils.data.dataloader.DataLoader at 0x7f7f0187c210>,\n",
       " 'test': <torch.utils.data.dataloader.DataLoader at 0x7f7f0187cd10>}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model downloaded: bert-base-uncased\n"
     ]
    }
   ],
   "source": [
    "# ESN parameters\n",
    "esn_params = {\n",
    "            'embedding': 'bert-base-uncased', # TEXT.vocab.vectors,\n",
    "            'distribution' : 'uniform',               # uniform, gaussian\n",
    "            'input_dim' : 768,                        # dim of BERT encoding!\n",
    "            'reservoir_dim' : 1000,\n",
    "            'bias_scaling' : 0., #1.0742377381236705, # 1.0,\n",
    "            'sparsity' : 0.,\n",
    "            'spectral_radius' : 0.7094538192983408, # 0.9,\n",
    "            'leaking_rate': 0.17647315261153904, # 0.5,\n",
    "            'activation_function' : 'relu',\n",
    "            'input_scaling' : 0.1, # 1.0,\n",
    "            'mean' : 0.0,\n",
    "            'std' : 1.0,\n",
    "            'learning_algo' : None,     # initialzed below\n",
    "            'criterion' : None,         # initialzed below\n",
    "            'optimizer' : None,         # initialzed below\n",
    "            'pooling_strategy' : 'mean',\n",
    "            'bidirectional' : False,     # True\n",
    "            'device' : device,\n",
    "            'mode' : 'esn',              # 'no_layer, 'linear_layer'\n",
    "            'seed' : 42345\n",
    "             }\n",
    "\n",
    "# Instantiate the ESN\n",
    "ESN = esn.EchoStateNetwork(**esn_params)\n",
    "\n",
    "# Define the learning algo of the ESN\n",
    "# Ridge Regression\n",
    "ESN.learning_algo = la.RidgeRegression(alpha=7.843536845714804)\n",
    "# ESN.learning_algo = la.RidgeRegression2(alpha=7.843536845714804)\n",
    "# ESN.learning_algo = la.LinearSVC()\n",
    "\n",
    "# Logistic Regression (uncomment below)\n",
    "# if esn_params['mode'] == 'no_layer':\n",
    "#     input_dim = esn_params['input_dim']\n",
    "# else:\n",
    "#     input_dim = esn_params['reservoir_dim']\n",
    "    \n",
    "# ESN.learning_algo = la.LogisticRegression(input_dim=input_dim, output_dim=6)\n",
    "# ESN.criterion = torch.nn.CrossEntropyLoss()                                  # loss\n",
    "# ESN.optimizer = torch.optim.Adam(ESN.learning_algo.parameters(), lr=0.01)    # optimizer\n",
    "\n",
    "# ESN.learning_algo = la.LogisticRegression2()\n",
    "\n",
    "# Put the ESN on the device (CPU or GPU)\n",
    "ESN = ESN.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warm up the ESN on multiple sentences\n",
    "nb_sentences = 10\n",
    "\n",
    "for i in range(nb_sentences): \n",
    "    sentence = dataset_d[\"train\"].select([i])\n",
    "    dataloader_tmp = torch.utils.data.DataLoader(sentence, \n",
    "                                                 batch_size=1, \n",
    "                                                 collate_fn=DataCollatorWithPadding(tokenizer))  \n",
    "\n",
    "    for sentence in dataloader_tmp:\n",
    "        ESN.warm_up(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the ESN\n",
    "# ESN.fit(dataloader_d[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # training the ESN (Logistic Regression, gradient descent)\n",
    "ESN.fit(dataloader_d[\"train\"], epochs=2, iter_steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train predictions and accuracy\n",
    "train_pred, train_acc = ESN.predict(dataloader_d[\"train\"], verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98.44036697247707"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_acc.item() if device.type == 'cuda' else train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test predictions and accuracy\n",
    "test_pred, test_acc = ESN.predict(dataloader_d[\"test\"], verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc.item() if device.type == 'cuda' else test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8462    0.9167    0.8800        12\n",
      "           1     0.8000    0.6667    0.7273         6\n",
      "           3     0.6250    1.0000    0.7692         5\n",
      "           4     0.9375    0.9375    0.9375        16\n",
      "           5     1.0000    0.7273    0.8421        11\n",
      "\n",
      "    accuracy                         0.8600        50\n",
      "   macro avg     0.8417    0.8496    0.8312        50\n",
      "weighted avg     0.8816    0.8600    0.8607        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test classification report\n",
    "print(classification_report(test_pred.tolist(), \n",
    "                            dataset_d['test']['labels'].tolist(), \n",
    "                            digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
