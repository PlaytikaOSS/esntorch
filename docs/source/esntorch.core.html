

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>esntorch.core package &mdash; esntorch 1.0.5 documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />

  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home" alt="Documentation Home"> esntorch
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../esn_description.html">Echo State Networks for Text Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorial_DeepESN.html">Tutorial 2: Deep Echo State Networks for Text Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api.html">Architecture</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">esntorch</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>esntorch.core package</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/source/esntorch.core.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="esntorch-core-package">
<h1>esntorch.core package<a class="headerlink" href="#esntorch-core-package" title="Permalink to this headline">¶</a></h1>
<section id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this headline">¶</a></h2>
</section>
<section id="module-esntorch.core.reservoir">
<span id="esntorch-core-reservoir-module"></span><h2>esntorch.core.reservoir module<a class="headerlink" href="#module-esntorch.core.reservoir" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="esntorch.core.reservoir.DeepLayer">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">esntorch.core.reservoir.</span></span><span class="sig-name descname"><span class="pre">DeepLayer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">nb_layers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#esntorch.core.reservoir.DeepLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#esntorch.core.reservoir.Layer" title="esntorch.core.reservoir.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">esntorch.core.reservoir.Layer</span></code></a></p>
<p>Implements a deep layer, to be used in the context of a deep ESN.
Parameters are self-explanatory.</p>
<p>Implements a <strong>deep layer</strong> of a network.
A deep layer is composed of an <strong>embedding</strong> and a succession of  <strong>reservoir</strong> of neurons:</p>
<p>DEEP LAYER = EMBEDDING + RESERVOIR + RESERVOIR + … + RESERVOIR.</p>
<p>This layer takes a tokenized text as input,
embeds it, and pass it through the successive reservoirs (possibly in both directions).
This class inherits from <cite>Layer</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>nb_layers</strong> (<cite>int</cite>) – Number of layers (reservoirs).</p></li>
<li><p><strong>**kwargs</strong> (<em>optional</em>) – Other keyword arguments.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="esntorch.core.reservoir.DeepLayer.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="pre">:</span> <span class="pre">bool</span></em><a class="headerlink" href="#esntorch.core.reservoir.DeepLayer.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="esntorch.core.reservoir.DeepLayer.warm_up">
<span class="sig-name descname"><span class="pre">warm_up</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">warm_up_sequence</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#esntorch.core.reservoir.DeepLayer.warm_up" title="Permalink to this definition">¶</a></dt>
<dd><p>Warms up the deep ESN.
Passes successive sequences through the deep ESN and updates the initial state of all reservoirs.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>warm_up_sequence</strong> (<em>transformers.tokenization_utils_base.BatchEncoding</em>) – batch of sentences used for the warm up.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="esntorch.core.reservoir.Layer">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">esntorch.core.reservoir.</span></span><span class="sig-name descname"><span class="pre">Layer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">embedding</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">seed</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">42</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">device(type='cpu')</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#esntorch.core.reservoir.Layer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Implements a <strong>layer</strong> of a network.
A general layer is composed of an <strong>embedding</strong> and a <strong>reservoir</strong> of neurons:</p>
<p>LAYER = EMBEDDING + RESERVOIR.</p>
<p>The layer takes a tokenized text as input, embeds it, and pass it through the reservoir
(possibly in both directions).
Sometimes, the embedding can be omitted (case of deep ESNs)
and the reservoir can also be omitted (case of a baseline).
This class a base class for more complex layers: <code class="docutils literal notranslate"><span class="pre">LayerLinear</span></code>, <code class="docutils literal notranslate"><span class="pre">LayerRecurrent</span></code>, <code class="docutils literal notranslate"><span class="pre">DeepLayer</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>embedding</strong> (<cite>None</cite> or <cite>str</cite>) – Name of Hugging Face model used for embedding or None.
The None case is used to implement deep ESNs, i.e.,
cases in which the inputs are given by a previous reservoir instead of an embedding.</p></li>
<li><p><strong>input_dim</strong> (<cite>int</cite>) – Dimension of the inputs.
If a Hugging Face model is given as an embedding,
then input_dim is automatically set to the dimension of this model.
Otherwise, input_dim needs to be specified, cf. case of deep ESNs (DeepLayer).</p></li>
<li><p><strong>seed</strong> (<cite>int</cite>) – Random seed.</p></li>
<li><p><strong>device</strong> (<cite>torch.device</cite>) – The device to be used: cpu or gpu.</p></li>
<li><p><strong>**kwargs</strong> (<em>optional</em>) – Other keyword arguments.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="esntorch.core.reservoir.Layer.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#esntorch.core.reservoir.Layer.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements the forward pass per se.
Passes the input batch through the reservoir (using the <code class="docutils literal notranslate"><span class="pre">_forward</span></code> method).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>batch</strong> (<cite>transformers.tokenization_utils_base.BatchEncoding</cite> or <cite>torch.Tensor</cite>) – Input batch to be processed.
Either a Hugging Face batch of tokenized texts;
Or a 3d tensor of states [max length x batch size x embedding dim]</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>(states, lengths):
(i) states is a 3D tensor [batch size x max length x embedding dim] containing the reservoir states.
(ii) lengths is a 1D tensor [batch size].</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><cite>tuple</cite> [<cite>torch.Tensor</cite>, <cite>torch.Tensor</cite>]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="esntorch.core.reservoir.Layer.reverse_forward">
<span class="sig-name descname"><span class="pre">reverse_forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#esntorch.core.reservoir.Layer.reverse_forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements the forward pass, but with the texts or states in the reversed order.
Reverses the inout batch and passes through the reservoir (using the <code class="docutils literal notranslate"><span class="pre">forward</span></code> method).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>batch</strong> (<cite>transformers.tokenization_utils_base.BatchEncoding</cite> or <cite>torch.Tensor</cite>) – Input batch to be processed.
Either a Hugging Face batch of tokenized texts;
Or a 3d tensor of states [max length x batch size x embedding dim]</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>(states, lengths):
(i) states is a 3D tensor [batch size x max length x embedding dim] containing the reservoir states.
(ii) lengths is a 1D tensor [batch size].</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><cite>tuple</cite> [<cite>torch.Tensor</cite>, <cite>torch.Tensor</cite>]</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="esntorch.core.reservoir.Layer.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="pre">:</span> <span class="pre">bool</span></em><a class="headerlink" href="#esntorch.core.reservoir.Layer.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="esntorch.core.reservoir.Layer.warm_up">
<span class="sig-name descname"><span class="pre">warm_up</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">warm_up_sequence</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_states</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#esntorch.core.reservoir.Layer.warm_up" title="Permalink to this definition">¶</a></dt>
<dd><p>Warms up the ESN (in the case its reservoir is built with a <code class="docutils literal notranslate"><span class="pre">LayerRecurrent</span></code>).
Passes successive sequences through the ESN and updates its initial state.
In this case, there is no reservoir, hence nothing is done.
This method will be overwritten by the next children classes.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>warm_up_sequence</strong> (<em>transformers.tokenization_utils_base.BatchEncoding</em>) – batch of sentences used for the warm up.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="esntorch.core.reservoir.LayerLinear">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">esntorch.core.reservoir.</span></span><span class="sig-name descname"><span class="pre">LayerLinear</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_scaling</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias_scaling</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation_function</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'tanh'</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#esntorch.core.reservoir.LayerLinear" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#esntorch.core.reservoir.Layer" title="esntorch.core.reservoir.Layer"><code class="xref py py-class docutils literal notranslate"><span class="pre">esntorch.core.reservoir.Layer</span></code></a></p>
<p>Implements a <strong>linear layer</strong> of a network.
A linear layer is composed of an <strong>embedding</strong> and a <strong>linear reservoir</strong> of neurons:</p>
<p>LINEAR LAYER = EMBEDDING + LINEAR RESERVOIR.</p>
<p>This layer takes a tokenized text as input,
embeds it, and pass it through a linear layer - or an non-recurrent reservoir -
(possibly in both directions).
This class inherits from <cite>Layer</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dim</strong> (<cite>int</cite>) – Dimension of the layer.</p></li>
<li><p><strong>input_scaling</strong> (<cite>float</cite>) – Input scaling: bounds of the uniform distribution from which the input weights are drawn.</p></li>
<li><p><strong>bias_scaling</strong> (<cite>float</cite>) – Bias scaling: bounds of the uniform distribution from which the biases are drawn.</p></li>
<li><p><strong>activation_function</strong> (<cite>str</cite>) – Activation function of the neurons: either <cite>tanh</cite> or <cite>relu</cite>.</p></li>
<li><p><strong>**kwargs</strong> (<em>optional</em>) – Other keyword arguments.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="esntorch.core.reservoir.LayerLinear.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="pre">:</span> <span class="pre">bool</span></em><a class="headerlink" href="#esntorch.core.reservoir.LayerLinear.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="esntorch.core.reservoir.LayerRecurrent">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">esntorch.core.reservoir.</span></span><span class="sig-name descname"><span class="pre">LayerRecurrent</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">distribution</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'gaussian'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mean</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">std</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sparsity</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">spectral_radius</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">leaking_rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#esntorch.core.reservoir.LayerRecurrent" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#esntorch.core.reservoir.LayerLinear" title="esntorch.core.reservoir.LayerLinear"><code class="xref py py-class docutils literal notranslate"><span class="pre">esntorch.core.reservoir.LayerLinear</span></code></a></p>
<p>Implements a <strong>recurrent layer</strong> of a network.
A recurrent layer is composed of an <strong>embedding</strong> and a (recurrent) <strong>reservoir</strong> of neurons:</p>
<p>RECURRENT LAYER = EMBEDDING + RESERVOIR.</p>
<p>This layer takes a tokenized text as input,
embeds it, and pass it through a recurrent reservoir (possibly in both directions).
This class inherits from <cite>Layer</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>distribution</strong> (<cite>str</cite>) – Distribution from which the reservoir weights are drawn: ‘uniform’ or ‘gaussian’.</p></li>
<li><p><strong>mean</strong> (<cite>float</cite>) – If Gaussian, mean of the distribution; ignored otherwise.</p></li>
<li><p><strong>std</strong> (<cite>float</cite>) – If Gaussian, standard deviation of the distribution; ignored otherwise.</p></li>
<li><p><strong>sparsity</strong> (<cite>float</cite>) – Number between 0 and 1 representing the percentage of reservoir weights that are 0.</p></li>
<li><p><strong>spectral_radius</strong> (<cite>float</cite>) – Spectral radius of the reservoir weight matrix.</p></li>
<li><p><strong>leaking_rate</strong> – Leaking rate of the network: between 0 and 1.</p></li>
<li><p><strong>**kwargs</strong> (<em>optional</em>) – Other keyword arguments.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="esntorch.core.reservoir.LayerRecurrent.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="pre">:</span> <span class="pre">bool</span></em><a class="headerlink" href="#esntorch.core.reservoir.LayerRecurrent.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="esntorch.core.reservoir.LayerRecurrent.warm_up">
<span class="sig-name descname"><span class="pre">warm_up</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">warm_up_sequence</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#esntorch.core.reservoir.LayerRecurrent.warm_up" title="Permalink to this definition">¶</a></dt>
<dd><p>Warms up the ESN.
Passes successive sequences through the ESN and updates its initial state.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>warm_up_sequence</strong> (<em>transformers.tokenization_utils_base.BatchEncoding</em>) – batch of sentences used for the warm up.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="esntorch.core.reservoir.create_layer">
<span class="sig-prename descclassname"><span class="pre">esntorch.core.reservoir.</span></span><span class="sig-name descname"><span class="pre">create_layer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'recurrent_layer'</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#esntorch.core.reservoir.create_layer" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a recurrent, linear or empty layer depending on the given mode.
The possible modes are: <code class="docutils literal notranslate"><span class="pre">'recurrent_layer'</span></code>,  <code class="docutils literal notranslate"><span class="pre">'linear_layer'</span></code>, and <code class="docutils literal notranslate"><span class="pre">'no_layer'</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>mode</strong> (<cite>str</cite>) – </p></li>
<li><p><strong>**kwargs</strong> (<em>optional</em>) – Other keyword arguments.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The layer built.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><cite>Layer</cite> or <cite>ValueError</cite></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="esntorch.core.reservoir.get_parameters">
<span class="sig-prename descclassname"><span class="pre">esntorch.core.reservoir.</span></span><span class="sig-name descname"><span class="pre">get_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">nb_layers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">index</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#esntorch.core.reservoir.get_parameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Given arguments for a deep ESN with n reservoirs, retrieves the arguments for the i-th reservoir,
where n = <code class="docutils literal notranslate"><span class="pre">nb_layers</span></code> and i = <code class="docutils literal notranslate"><span class="pre">index</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>nb_layers</strong> (<cite>int</cite>) – </p></li>
<li><p><strong>index</strong> (<cite>int</cite>) – </p></li>
<li><p><strong>**kwargs</strong> (<em>optional</em>) – Other keyword arguments.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>new_kwargs</strong> – Dictionary of arguments for the i-th reservoir of a deep ESN with n reservoirs,
where i = index and n = nb_layers.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><cite>dict</cite></p>
</dd>
</dl>
</dd></dl>

</section>
<section id="module-esntorch.core.pooling_strategy">
<span id="esntorch-core-pooling-strategy-module"></span><h2>esntorch.core.pooling_strategy module<a class="headerlink" href="#module-esntorch.core.pooling_strategy" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="esntorch.core.pooling_strategy.Pooling">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">esntorch.core.pooling_strategy.</span></span><span class="sig-name descname"><span class="pre">Pooling</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">pooling_strategy</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weights</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lexicon</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#esntorch.core.pooling_strategy.Pooling" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Implements various pooling strategies - or pooling layers - for merging successive ESN states.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pooling_strategy</strong> (<cite>None</cite> or <cite>str</cite>) – The possible pooling strategies are:
None, ‘first’, ‘last’, mean’, ‘weighted’, ‘lexicon_weighted’.
None: collects all the ESN states (no pooling).
‘first’: takes the first ESN state as the merged state.
‘last’: takes the last ESN state as the merged state.
‘mean’: takes the mean of ESN states as the merged state.
‘weighted’: takes a weighted mean of ESN states as the merged state.
‘lexicon_weighted’: takes a weighted mean of ESN states as the merged state.
The weights of the words are given by a lexicon.</p></li>
<li><p><strong>weights</strong> (<cite>None</cite> or <cite>torch.Tensor</cite>) – Weights to be considered in the case of ‘weighted’ pooling.
If weights is None (default), computes attention-like weights.
The weight of each state is computed based on its norm.
If weights != None, uses the given weights.</p></li>
<li><p><strong>lexicon</strong> (<cite>None</cite> or <cite>torch.Tensor</cite>) – Weights to be considered in the case of ‘lexicon_weighted’ pooling.
If not None, 1D tensor containing the lexicon weight of each word id.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="esntorch.core.pooling_strategy.Pooling.merge_batch">
<span class="sig-name descname"><span class="pre">merge_batch</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">states</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lengths</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">texts</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pooling_strategy</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weights</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">additional_fts</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#esntorch.core.pooling_strategy.Pooling.merge_batch" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements different pooling strategies: None, ‘first’, ‘last’, ‘mean’, ‘weighted’.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>states</strong> (<cite>torch.Tensor</cite>) – 3D tensor containing a batch of ESN states: [batch size x  max text length x layer dim].</p></li>
<li><p><strong>lengths</strong> (<cite>torch.Tensor</cite>) – 1D tensor containing the lengths of each sentence in the batch: [batch size].</p></li>
<li><p><strong>texts</strong> (<cite>transformers.tokenization_utils_base.BatchEncoding</cite>) – Batch of token ids.</p></li>
<li><p><strong>pooling_strategy</strong> (<cite>None</cite> or <cite>str</cite>) – Possible values are: None, ‘first’, ‘last’, ‘mean’, ‘weighted’, ‘lexicon_weighted’.</p></li>
<li><p><strong>weights</strong> (<cite>None</cite> or <cite>torch.Tensor</cite>) – 2D tensor containing the weights for each state [batch size x max text length].</p></li>
<li><p><strong>additional_fts</strong> (<cite>None</cite> or <cite>torch.Tensor</cite>) – 2D tensor containing new features (e.g. tf-idf) to be concatenated to each merged state
[batch size x dim + additional_fts].</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>merged_states</strong> – If pooling_strategy is not None, 2D tensor containing the merged states [batch size x layer dim].
If pooling_strategy is None, 2D tensor containing all the states [Sum_i len(sentence_i) x layer dim].</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><cite>torch.Tensor</cite></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-esntorch.core.learning_algo">
<span id="esntorch-core-learning-algo-module"></span><h2>esntorch.core.learning_algo module<a class="headerlink" href="#module-esntorch.core.learning_algo" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="esntorch.core.learning_algo.DeepNN">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">esntorch.core.learning_algo.</span></span><span class="sig-name descname"><span class="pre">DeepNN</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">layers_l</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#esntorch.core.learning_algo.DeepNN" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Implements a deep neural network whose layers are specified in a list.
Make sure that the dimensions of the first and last layers
correspond to the input and output dimensions, respectively.
Example: A 2-hidden layer NN with 50 neurons in each layer and input / output dimensions of 300 / 3
is implemented as follows:
layers_l = [300, 50, 50, 3];
model = DeepNN(layers_l).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>layers_l</strong> (<cite>list</cite> [<cite>int</cite>]) – List of integers representing the number on neurons in each layer.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="esntorch.core.learning_algo.DeepNN.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">activation</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#esntorch.core.learning_algo.DeepNN.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements the forward pass with ReLU activation function for the hidden layers.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>activation</strong> (<cite>torch.Tensor</cite>) – Activation values of the input layer neurons: (batch size x input dim).</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>activation</strong> – Activation values of the output layer neurons: (batch size x output dim).</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><cite>torch.Tensor</cite></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="esntorch.core.learning_algo.DeepNN.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="pre">:</span> <span class="pre">bool</span></em><a class="headerlink" href="#esntorch.core.learning_algo.DeepNN.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="esntorch.core.learning_algo.LinearSVC">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">esntorch.core.learning_algo.</span></span><span class="sig-name descname"><span class="pre">LinearSVC</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">penalty</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'l2'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'squared_hinge'</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dual</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">C</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">multi_class</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'ovr'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fit_intercept</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">intercept_scaling</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">class_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_iter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1000</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#esntorch.core.learning_algo.LinearSVC" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Implements Linear Support Vector Machine Classifier LinearSVC from scikit learn.
Takes the same parameters as those of sklearn.svm.LinearSVC:
<a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html">https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html</a></p>
<dl class="py method">
<dt class="sig sig-object py" id="esntorch.core.learning_algo.LinearSVC.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#esntorch.core.learning_algo.LinearSVC.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Overrides the fit method of LinearSVC.
Simply converts torch tensors into numpy arrays, and applies original fit method.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<cite>torch.Tensor</cite>) – Tensor of features (gathered by rows).</p></li>
<li><p><strong>y</strong> (<cite>torch.Tensor</cite>) – Tensor of targets (gathered by rows).</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="esntorch.core.learning_algo.LogisticRegression">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">esntorch.core.learning_algo.</span></span><span class="sig-name descname"><span class="pre">LogisticRegression</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input_dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_dim</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#esntorch.core.learning_algo.LogisticRegression" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Implements classical logistic regression as a 1-layer neural network.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_dim</strong> (<cite>int</cite>) – Input dimension.</p></li>
<li><p><strong>output_dim</strong> (<cite>int</cite>) – Output dimension.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="esntorch.core.learning_algo.LogisticRegression.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#esntorch.core.learning_algo.LogisticRegression.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements the forward pass.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>x</strong> (<cite>torch.Tensor</cite>) – Tensor of features (gathered by rows).</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>outputs</strong> – Tensor of outputs (gathered by row).</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><cite>torch.Tensor</cite></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="esntorch.core.learning_algo.LogisticRegression.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="pre">:</span> <span class="pre">bool</span></em><a class="headerlink" href="#esntorch.core.learning_algo.LogisticRegression.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="esntorch.core.learning_algo.LogisticRegression_skl">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">esntorch.core.learning_algo.</span></span><span class="sig-name descname"><span class="pre">LogisticRegression_skl</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">penalty</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'l2'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dual</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">C</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fit_intercept</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">intercept_scaling</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">class_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">solver</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'liblinear'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_iter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">multi_class</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">warm_start</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_jobs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">l1_ratio</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#esntorch.core.learning_algo.LogisticRegression_skl" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Implements the Logistic Regression LogisticRegression from scikit learn.
Takes the same parameters as those of sklearn.linear_model.LogisticRegression
<a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html">https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html</a></p>
<dl class="py method">
<dt class="sig sig-object py" id="esntorch.core.learning_algo.LogisticRegression_skl.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#esntorch.core.learning_algo.LogisticRegression_skl.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Overrides the fit method of LogisticRegression.
Simply converts torch tensors into numpy arrays. and applies original fit method.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<cite>torch.Tensor</cite>) – Tensor of features (gathered by rows).</p></li>
<li><p><strong>y</strong> (<cite>torch.Tensor</cite>) – Tensor of targets (gathered by rows).</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="esntorch.core.learning_algo.RidgeRegression">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">esntorch.core.learning_algo.</span></span><span class="sig-name descname"><span class="pre">RidgeRegression</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#esntorch.core.learning_algo.RidgeRegression" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Implements Ridge regression via its closed form solution:
$$ beta = (X^T X + lambda I)^{-1} X^T y $$
Works for multi-class problems with target values 0, 1, 2, etc.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>alpha</strong> (<cite>float</cite>) – </p></li>
<li><p><strong>mode</strong> (<cite>None</cite> or <cite>str</cite>) – Default mode is None.
The modes ‘normalize’ and ‘standardize’ are also possible.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="esntorch.core.learning_algo.RidgeRegression.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#esntorch.core.learning_algo.RidgeRegression.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the closed form solution of the Ridge regression
and update the learned weights.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<cite>torch.Tensor</cite>) – Tensor of features (gathered by rows).</p></li>
<li><p><strong>y</strong> (<cite>torch.Tensor</cite>) – Tensor of targets (gathered by rows).</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="esntorch.core.learning_algo.RidgeRegression.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#esntorch.core.learning_algo.RidgeRegression.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes predictions.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<cite>torch.Tensor</cite>) – Tensor of features (gathered by rows).</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>outputs</strong> – Outputs of Ridge regression</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><cite>torch.Tensor</cite></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="esntorch.core.learning_algo.RidgeRegression.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="pre">:</span> <span class="pre">bool</span></em><a class="headerlink" href="#esntorch.core.learning_algo.RidgeRegression.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="esntorch.core.learning_algo.RidgeRegression_skl">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">esntorch.core.learning_algo.</span></span><span class="sig-name descname"><span class="pre">RidgeRegression_skl</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fit_intercept</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">normalize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'deprecated'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">copy_X</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_iter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">solver</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">positive</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#esntorch.core.learning_algo.RidgeRegression_skl" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Implements the Ridge Regression from scikit learn.
Takes the parameters as those of sklearn.linear_model.Ridge
<a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeClassifier.html">https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeClassifier.html</a></p>
<dl class="py method">
<dt class="sig sig-object py" id="esntorch.core.learning_algo.RidgeRegression_skl.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#esntorch.core.learning_algo.RidgeRegression_skl.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements the fit method of RidgeClassifier.
Simply converts torch tensors into numpy arrays, and applies original fit method.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<cite>torch.Tensor</cite>) – Tensor of features (gathered by rows).</p></li>
<li><p><strong>y</strong> (<cite>torch.Tensor</cite>) – Tensor of targets (gathered by rows).</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-esntorch.core.esn">
<span id="esntorch-core-esn-module"></span><h2>esntorch.core.esn module<a class="headerlink" href="#module-esntorch.core.esn" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="esntorch.core.esn.EchoStateNetwork">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">esntorch.core.esn.</span></span><span class="sig-name descname"><span class="pre">EchoStateNetwork</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">learning_algo</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">criterion</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pooling_strategy</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bidirectional</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lexicon</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">deep</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">device(type='cpu')</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#esntorch.core.esn.EchoStateNetwork" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Implements an <strong>echo state Nnetwork (ESN)</strong> per se.
An ESN consists of the combination of
a <strong>layer</strong>, a <strong>pooling strategy</strong> and a <strong>learning algorithm</strong>:</p>
<p>ESN = LAYER + POOLING + LEARNING ALGO.</p>
<p>Recalling that a general <strong>layer</strong> is itslef composed of
an <strong>embedding</strong> and a <strong>reservoir</strong> of neurons, one finally has:</p>
<p>ESN = EMBEDDING + RESERVOIR + POOLING + LEARNING ALGO.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>learning_algo</strong> (<cite>object</cite>) – A learning algo used to train the netwok (see learning_algo.py).</p></li>
<li><p><strong>criterion</strong> (<cite>torch.nn.Module</cite> or <cite>None</cite>) – Pytorch loss used to train the ESN (in case of non-direct methods).</p></li>
<li><p><strong>optimizer</strong> (<cite>torch.optim.Optimizer</cite> or <cite>None</cite>) – Pytorch optimizer used to train the ESN (in case of non-direct methods).</p></li>
<li><p><strong>pooling_strategy</strong> (<cite>str</cite>) – Pooling strategy to be used: ‘mean’, ‘first’, ‘last’,
‘weighted’, ‘lexicon_weighted’, None,</p></li>
<li><p><strong>bidirectional</strong> (<cite>bool</cite>) – Whether to implement a bi-directional ESN or not,</p></li>
<li><p><strong>lexicon</strong> (<cite>torch.Tensor</cite> or <cite>None</cite>) – If not None, lexicon to be used with the pooling strategy ‘lexicon_weighted’.</p></li>
<li><p><strong>deep</strong> (<cite>bool</cite>) – Whether to implement a deep ESN or not.</p></li>
<li><p><strong>device</strong> (<cite>int</cite>) – The device to be used: cpu or gpu.</p></li>
<li><p><strong>**kwargs</strong> (<em>optional</em>) – Other keyword arguments.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="esntorch.core.esn.EchoStateNetwork.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">train_dataloader</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epochs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">iter_steps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#esntorch.core.esn.EchoStateNetwork.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fits ESN.
Calls the correct <code class="docutils literal notranslate"><span class="pre">_fit_direct()</span></code> or <code class="docutils literal notranslate"><span class="pre">_fit_GD()</span></code> method
depending on the learning algorithm.
The parameters <code class="docutils literal notranslate"><span class="pre">epochs</span></code> and <code class="docutils literal notranslate"><span class="pre">iter_steps</span></code> are ignored
in the case of a <code class="docutils literal notranslate"><span class="pre">_fit_direct</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>train_dataloader</strong> (<cite>torch.utils.data.dataloader.DataLoader</cite>) – Training dataloader.</p></li>
<li><p><strong>epochs</strong> (<cite>int</cite>) – Number of training epochs.</p></li>
<li><p><strong>iter_steps</strong> (<cite>int</cite>) – Number of steps (batches) after which the loss is printed.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="esntorch.core.esn.EchoStateNetwork.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dataloader</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#esntorch.core.esn.EchoStateNetwork.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Evaluates the ESN on a dataloader (train, test, validation).
Returns the list of prediction labels.
If the true labels are known, then returns the accuracy also.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dataloader</strong> (<cite>torch.utils.data.dataloader.DataLoader</cite>) – Dataloader.</p></li>
<li><p><strong>verbose</strong> (<cite>bool</cite>) – </p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>(predictions, accuracy): list of predictions and accuracy.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><cite>tuple</cite> [<cite>list</cite>, <cite>float</cite>]</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="esntorch.core.esn.EchoStateNetwork.training">
<span class="sig-name descname"><span class="pre">training</span></span><em class="property"><span class="pre">:</span> <span class="pre">bool</span></em><a class="headerlink" href="#esntorch.core.esn.EchoStateNetwork.training" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="esntorch.core.esn.EchoStateNetwork.warm_up">
<span class="sig-name descname"><span class="pre">warm_up</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dataset</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#esntorch.core.esn.EchoStateNetwork.warm_up" title="Permalink to this definition">¶</a></dt>
<dd><p>Warms up the ESN (in the case its reservoir is built with a <code class="docutils literal notranslate"><span class="pre">LayerRecurrent</span></code>).
Passes successive sequences through the ESN and updates its initial state.
In this case, there is no reservoir, hence nothing is done.
This method will be overwritten by the next children classes.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>dataset</strong> (<cite>datasets.arrow_dataset.Dataset</cite>) – Datasets of sentences to be used for the warming.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-esntorch.core">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-esntorch.core" title="Permalink to this headline">¶</a></h2>
</section>
</section>


           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright 2021, Playtika Ltd.

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>