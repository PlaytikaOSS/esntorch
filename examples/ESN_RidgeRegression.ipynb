{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1: ESN with Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides a use case example of the ``EsnTorch`` library. It describes the implementation of an **Echo State Network (ESN)** for text classification on the **TREC-6** dataset (question classification).\n",
    "\n",
    "The instantiation, training and evaluation of an ESN for text classification\n",
    "is achieved via the following steps:\n",
    "- Import the required libraries and modules\n",
    "- Load and prepare data\n",
    "- Instantiate the model by:\n",
    "    - specifying its constituting parameters\n",
    "    - specifying its learning algorithm\n",
    "    - warming up the model\n",
    "- Train the ESN\n",
    "- Evaluate the ESN on the train and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers==4.8.2\n",
    "# !pip install datasets==1.7.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment this if library is installed!\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath(\"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import torch\n",
    "\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers.data.data_collator import DataCollatorWithPadding\n",
    "\n",
    "import esntorch.core.reservoir as res\n",
    "import esntorch.core.learning_algo as la\n",
    "import esntorch.core.merging_strategy as ms\n",
    "import esntorch.core.esn as esn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set device (cpu or gpu if available)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and tokenize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom functions for loading and preparing data\n",
    "def tokenize(sample):\n",
    "    \"\"\"Tokenize sample\"\"\"\n",
    "    \n",
    "    sample = tokenizer(sample['text'], truncation=True, padding=False, return_length=True)\n",
    "    \n",
    "    return sample\n",
    "    \n",
    "def load_and_prepare_dataset(dataset_name, split, cache_dir):\n",
    "    \"\"\"\n",
    "    Load dataset from the datasets library of HuggingFace.\n",
    "    Tokenize and add length.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load dataset\n",
    "    dataset = load_dataset(dataset_name, split=split, cache_dir=CACHE_DIR)\n",
    "    \n",
    "    # Rename label column for tokenization purposes (use 'label-fine' for fine-grained labels)\n",
    "    dataset = dataset.rename_column('label-coarse', 'labels')\n",
    "    \n",
    "    # Tokenize data\n",
    "    dataset = dataset.map(tokenize, batched=True)\n",
    "    dataset = dataset.rename_column('length', 'lengths')\n",
    "    dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels', 'lengths'])\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset trec (cache_dir/trec/default/1.1.0/751da1ab101b8d297a3d6e9c79ee9b0173ff94c4497b75677b59b61d5467a9b9)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81434c64b426410581801129c3c6e737",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac2509ee3ed04da8b4741d6f138130fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load BERT tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load and prepare data\n",
    "CACHE_DIR = 'cache_dir/' # put your path here\n",
    "\n",
    "full_dataset = load_and_prepare_dataset('trec', split=None, cache_dir=CACHE_DIR)\n",
    "train_dataset = full_dataset['train'].sort(\"lengths\")\n",
    "test_dataset = full_dataset['test'].sort(\"lengths\")\n",
    "\n",
    "# Create dict of all datasets\n",
    "dataset_d = {\n",
    "    'train': train_dataset,\n",
    "    'test': test_dataset\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': Dataset({\n",
       "     features: ['attention_mask', 'input_ids', 'label-fine', 'labels', 'lengths', 'text', 'token_type_ids'],\n",
       "     num_rows: 5452\n",
       " }),\n",
       " 'test': Dataset({\n",
       "     features: ['attention_mask', 'input_ids', 'label-fine', 'labels', 'lengths', 'text', 'token_type_ids'],\n",
       "     num_rows: 500\n",
       " })}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dict of dataloaders\n",
    "\n",
    "dataloader_d = {}\n",
    "\n",
    "for k, v in dataset_d.items():\n",
    "    dataloader_d[k] = torch.utils.data.DataLoader(v, batch_size=256, \n",
    "                                                  collate_fn=DataCollatorWithPadding(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': <torch.utils.data.dataloader.DataLoader at 0x15d9e8100>,\n",
       " 'test': <torch.utils.data.dataloader.DataLoader at 0x15d9e8b80>}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instanciate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ESN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most parameters are self-explanatory for a reader familiar with ESNs. Please refer to the documentation for further details. \n",
    "\n",
    "The ``mode`` parameter represents the type of reservoir to be considered:\n",
    "- ``recurrent_layer``: implements a **classical recurrent reservoir**, specified among others by its ``dim``, ``sparsity``, ``spectral_radius``, ``leaking_rate`` and ``activation_function``.\n",
    "- ``linear_layer``: implements a simple **linear layer** specified by its ``dim`` and ``activation_function``.\n",
    "- ``no_layer``: implements **the absence of reservoir**, meaning that the embedded inputs are directly fed to the the learning algorithms.\n",
    "\n",
    "The comparison between the ``recurrent_layer`` and the ``no_layer``modes enable to assess the impact of the reservoir on the results. The comparison between the ``recurrent_layer`` and the ``linear_layer``modes enable to assess the impact of the recurrence of the reservoir on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model downloaded: bert-base-uncased\n"
     ]
    }
   ],
   "source": [
    "# ESN parameters\n",
    "esn_params = {\n",
    "            'embedding_weights': 'bert-base-uncased',\n",
    "            'input_dim': 768,  # dim of BERT encoding!\n",
    "            'dim': 300,\n",
    "            'bias_scaling': 0.1,\n",
    "            'sparsity': 0.9,\n",
    "            'spectral_radius': 0.9,\n",
    "            'leaking_rate': 0.5,\n",
    "            'activation_function': 'tanh',\n",
    "            'input_scaling': 0.1,\n",
    "            'mean': 0.0,\n",
    "            'std': 1.0,     \n",
    "            'learning_algo': None,       # initialzed below\n",
    "            'criterion': None,           # initialzed below (only for learning algos trained with SGD)\n",
    "            'optimizer': None,           # initialzed below (only for learning algos trained with SGD)\n",
    "            'merging_strategy': 'mean',\n",
    "            'bidirectional': False,\n",
    "            'mode' : 'recurrent_layer',  # 'no_layer', 'linear_layer', 'recurrent_layer'\n",
    "            'device': device,\n",
    "            'seed': 42345\n",
    "            }\n",
    "\n",
    "# Instantiate the ESN\n",
    "ESN = esn.EchoStateNetwork(**esn_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge regression\n",
    "ESN.learning_algo = la.RidgeRegression(alpha=7.843536845714804)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ESN = ESN.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warm up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warm up the ESN on multiple sentences\n",
    "nb_sentences = 10\n",
    "\n",
    "for i in range(nb_sentences): \n",
    "    sentence = dataset_d[\"train\"].select([i])\n",
    "    dataloader_tmp = torch.utils.data.DataLoader(sentence, \n",
    "                                                 batch_size=1, \n",
    "                                                 collate_fn=DataCollatorWithPadding(tokenizer))  \n",
    "\n",
    "    for sentence in dataloader_tmp:\n",
    "        ESN.warm_up(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 33s, sys: 9.57 s, total: 2min 42s\n",
      "Wall time: 2min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# training the ESN\n",
    "ESN.fit(dataloader_d[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # training the ESN (Logistic Regression, gradient descent)\n",
    "# ESN.fit(dataloader_d[\"train\"], epochs=10, iter_steps=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'item'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/nb/lxmwgp25729ckv5bmrb53hww0000gp/T/ipykernel_28638/3753321060.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train predictions and accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrain_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mESN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader_d\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'item'"
     ]
    }
   ],
   "source": [
    "# Train predictions and accuracy\n",
    "train_pred, train_acc = ESN.predict(dataloader_d[\"train\"], verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train accuracy\n",
    "train_acc = train_acc.item() if device.type == 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test predictions\n",
    "test_pred, test_acc = ESN.predict(dataloader_d[\"test\"], verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test accuracy\n",
    "test_acc = test_acc.item() if device.type == 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test classification report\n",
    "print(classification_report(test_pred.tolist(), \n",
    "                            dataset_d['test']['labels'].tolist(), \n",
    "                            digits=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
