{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AGnews-4: TEXT Classification + BERT + Ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import re\n",
    "import pickle\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import BertModel\n",
    "from transformers.data.data_collator import DataCollatorWithPadding\n",
    "\n",
    "from ax import optimize\n",
    "from ax.plot.contour import plot_contour\n",
    "from ax.plot.trace import optimization_trace_single_method\n",
    "from ax.service.managed_loop import optimize\n",
    "from ax.utils.notebook.plotting import render, init_notebook_plotting\n",
    "\n",
    "import esntorch.core.reservoir as res\n",
    "import esntorch.core.learning_algo as la\n",
    "import esntorch.core.merging_strategy as ms\n",
    "import esntorch.core.esn as esn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_PATH = '~/Results/Ax_results/ESN' # path of your result folder\n",
    "CACHE_DIR = '~/Data/huggignface/'         # path of your  folder\n",
    "\n",
    "PARAMS_FILE = 'AGnews-4_params.pkl'\n",
    "RESULTS_FILE = 'AGnews-4_results.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename correct column as 'labels': depends on the dataset you load\n",
    "\n",
    "def load_and_enrich_dataset(dataset_name, split, cache_dir):\n",
    "    \n",
    "    dataset = load_dataset(dataset_name, split=split, cache_dir=CACHE_DIR)\n",
    "    \n",
    "    dataset = dataset.map(lambda e: tokenizer(e['text'], truncation=True, padding=False), batched=True)\n",
    "    dataset = dataset.rename_column('label', 'labels') # 'label-fine'\n",
    "    dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "    def add_lengths(sample):\n",
    "        sample[\"lengths\"] = sum(sample[\"input_ids\"] != 0)\n",
    "        return sample\n",
    "    \n",
    "    dataset = dataset.map(add_lengths, batched=False)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5972f35eb4144c008439cfabb72617a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1780.0, style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccdde28dbc6742749beb06fb2a54191c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1227.0, style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (/raid/home/jeremiec/huggingface_datasets/ag_news/default/0.0.0/0eeeaaa5fb6dffd81458e293dfea1adba2881ffcbdc3fb56baeb5a892566c29a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e869feef449043cdb4aea692d2f51d46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=120.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "415af33876b142b58189acf6d2ce76f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=120000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (/raid/home/jeremiec/huggingface_datasets/ag_news/default/0.0.0/0eeeaaa5fb6dffd81458e293dfea1adba2881ffcbdc3fb56baeb5a892566c29a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b72fc080bef14a388143fc0d0f551af1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0764259100b41b3a0c38b104f7e6c9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7600.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "CACHE_DIR = '/raid/home/jeremiec/huggingface_datasets'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "full_train_dataset = load_and_enrich_dataset('ag_news', split='train', cache_dir=CACHE_DIR).sort(\"lengths\") # toriving/sst5\n",
    "train_val_datasets = full_train_dataset.train_test_split(train_size=0.8, shuffle=True)\n",
    "train_dataset = train_val_datasets['train'].sort(\"lengths\")\n",
    "val_dataset = train_val_datasets['test'].sort(\"lengths\")\n",
    "\n",
    "test_dataset = load_and_enrich_dataset('ag_news', split='test', cache_dir=CACHE_DIR).sort(\"lengths\")\n",
    "\n",
    "dataset_d = {\n",
    "    'full_train': full_train_dataset,\n",
    "    'train': train_dataset,\n",
    "    'val': val_dataset,\n",
    "    'test': test_dataset\n",
    "    }\n",
    "\n",
    "dataloader_d = {}\n",
    "for k, v in dataset_d.items():\n",
    "    dataloader_d[k] = torch.utils.data.DataLoader(v, batch_size=256, collate_fn=DataCollatorWithPadding(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'full_train': Dataset({\n",
       "     features: ['attention_mask', 'input_ids', 'labels', 'lengths', 'text', 'token_type_ids'],\n",
       "     num_rows: 120000\n",
       " }),\n",
       " 'train': Dataset({\n",
       "     features: ['attention_mask', 'input_ids', 'labels', 'lengths', 'text', 'token_type_ids'],\n",
       "     num_rows: 96000\n",
       " }),\n",
       " 'val': Dataset({\n",
       "     features: ['attention_mask', 'input_ids', 'labels', 'lengths', 'text', 'token_type_ids'],\n",
       "     num_rows: 24000\n",
       " }),\n",
       " 'test': Dataset({\n",
       "     features: ['attention_mask', 'input_ids', 'labels', 'lengths', 'text', 'token_type_ids'],\n",
       "     num_rows: 7600\n",
       " })}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitness(leaking_rate, \n",
    "            spectral_radius, \n",
    "            input_scaling, \n",
    "            bias_scaling, \n",
    "            alpha, \n",
    "            reservoir_dim, \n",
    "            dataset_d, \n",
    "            dataloader_d, \n",
    "            seed_l=[1991, 420, 666, 1979, 7], # 5 seeds\n",
    "            return_test_acc=False):\n",
    "    \n",
    "    acc_l = []\n",
    "    time_l = []\n",
    "    \n",
    "    for seed in seed_l:\n",
    "    \n",
    "        # parameters\n",
    "        esn_params = {\n",
    "                    'embedding_weights': 'bert-base-uncased', # TEXT.vocab.vectors,\n",
    "                    'distribution' : 'uniform',               # uniform, gaussian\n",
    "                    'input_dim' : 768,                        # dim of encoding!\n",
    "                    'reservoir_dim' : reservoir_dim,\n",
    "                    'bias_scaling' : bias_scaling,\n",
    "                    'sparsity' : 0.99,\n",
    "                    'spectral_radius' : spectral_radius,\n",
    "                    'leaking_rate': leaking_rate,\n",
    "                    'activation_function' : 'tanh',\n",
    "                    'input_scaling' : input_scaling,\n",
    "                    'mean' : 0.0,\n",
    "                    'std' : 1.0,\n",
    "                    'learning_algo' : None,\n",
    "                    'criterion' : None,\n",
    "                    'optimizer' : None,\n",
    "                    'merging_strategy' : 'mean',\n",
    "                    'lexicon' : None,\n",
    "                    'bidirectional' : True, # False\n",
    "                    'device' : device,\n",
    "                    'seed' : seed\n",
    "                     }\n",
    "\n",
    "        # model\n",
    "        ESN = esn.EchoStateNetwork(**esn_params)\n",
    "\n",
    "        ESN.learning_algo = la.RidgeRegression(alpha = alpha)# , mode='normalize')\n",
    "\n",
    "        ESN = ESN.to(device)\n",
    "\n",
    "        # warm up (new)\n",
    "        nb_sentences = 3\n",
    "        for i in range(nb_sentences): \n",
    "\n",
    "            sentence = dataset_d[\"train\"].select([i])\n",
    "            dataloader_tmp = torch.utils.data.DataLoader(sentence, \n",
    "                                                         batch_size=1, \n",
    "                                                         collate_fn=DataCollatorWithPadding(tokenizer))  \n",
    "\n",
    "            for sentence in dataloader_tmp:\n",
    "                ESN.warm_up(sentence)\n",
    "        \n",
    "        # predict\n",
    "        if return_test_acc:\n",
    "            t0 = timer()\n",
    "            LOSS = ESN.fit(dataloader_d[\"full_train\"])\n",
    "            t1 = timer()\n",
    "            time_l.append(t1 - t0)\n",
    "            acc = ESN.predict(dataloader_d[\"test\"], verbose=False)[1].item()\n",
    "        else:\n",
    "            LOSS = ESN.fit(dataloader_d[\"train\"])\n",
    "            acc = ESN.predict(dataloader_d[\"val\"], verbose=False)[1].item()\n",
    "\n",
    "        acc_l.append(acc)\n",
    "        \n",
    "        # clean objects\n",
    "        del ESN.learning_algo\n",
    "        del ESN.criterion\n",
    "        del ESN.merging_strategy\n",
    "        del ESN\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    if return_test_acc:\n",
    "        return np.mean(acc_l), np.std(acc_l), np.mean(time_l), np.std(time_l)\n",
    "    else:\n",
    "        return np.mean(acc_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# fitness(leaking_rate=0.2, spectral_radius=1.1, input_scaling=0.8, bias_scaling=1.0, alpha=10, reservoir_dim=500, dataset_d=dataset_d, dataloader_d=dataloader_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrapped_fitness(d, return_test_acc=False):\n",
    "    \n",
    "    return fitness(leaking_rate=d['leaking_rate'],\n",
    "                   spectral_radius=d['spectral_radius'],\n",
    "                   input_scaling=d['input_scaling'],\n",
    "                   bias_scaling=d['bias_scaling'],\n",
    "                   alpha=d['alpha'],\n",
    "                   reservoir_dim=d['reservoir_dim'], # will be in the loop\n",
    "                   dataset_d=dataset_d,\n",
    "                   dataloader_d=dataloader_d,\n",
    "                   return_test_acc=return_test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** WARNING *** DO NO EXECUTE NEXT CELLS IF BIDIRECTIONAL MODE (OPTIM ALREADY DONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 05-30 21:40:50] ax.modelbridge.dispatch_utils: Using Bayesian Optimization generation strategy: GenerationStrategy(name='Sobol+GPEI', steps=[Sobol for 6 trials, GPEI for subsequent trials]). Iterations after 6 will take longer to generate due to  model-fitting.\n",
      "[INFO 05-30 21:40:50] ax.service.managed_loop: Started full optimization with 40 steps.\n",
      "[INFO 05-30 21:40:50] ax.service.managed_loop: Running optimization trial 1...\n",
      "[INFO 05-30 22:05:45] ax.service.managed_loop: Running optimization trial 2...\n",
      "[INFO 05-30 22:30:37] ax.service.managed_loop: Running optimization trial 3...\n"
     ]
    }
   ],
   "source": [
    "best_params_d = {}\n",
    "\n",
    "for res_dim in [500, 1000, 3000, 5000]:\n",
    "\n",
    "    best_parameters, best_values, experiment, model = optimize(\n",
    "            parameters=[\n",
    "              {\n",
    "                \"name\": \"leaking_rate\",\n",
    "                \"value_type\": \"float\",\n",
    "                \"type\": \"range\",\n",
    "                \"bounds\": [0.0, 0.999],\n",
    "              },\n",
    "              {\n",
    "                \"name\": \"spectral_radius\",\n",
    "                \"value_type\": \"float\",\n",
    "                \"type\": \"range\",\n",
    "                \"bounds\": [0.2, 1.7],\n",
    "              },\n",
    "              {\n",
    "                \"name\": \"input_scaling\",\n",
    "                \"value_type\": \"float\",\n",
    "                \"type\": \"range\",\n",
    "                \"bounds\": [0.1, 3.0],\n",
    "              },\n",
    "              {\n",
    "                \"name\": \"bias_scaling\",\n",
    "                \"value_type\": \"float\",\n",
    "                \"type\": \"range\",\n",
    "                \"bounds\": [0.1, 3.0],\n",
    "              },\n",
    "              {\n",
    "                \"name\": \"alpha\",\n",
    "                \"value_type\": \"float\",\n",
    "                \"type\": \"range\",\n",
    "                \"log_scale\": True,\n",
    "                \"bounds\": [1e-3, 1e3],\n",
    "              },\n",
    "              {\n",
    "                \"name\": \"reservoir_dim\",\n",
    "                \"value_type\": \"int\",\n",
    "                \"type\": \"fixed\",\n",
    "                \"value\": res_dim,\n",
    "              }\n",
    "            ],\n",
    "            # Booth function\n",
    "            evaluation_function = wrapped_fitness,\n",
    "            minimize = False,\n",
    "            objective_name = 'val_accuracy',\n",
    "            total_trials = 40\n",
    "        )\n",
    "    \n",
    "    # results\n",
    "    best_params_d[res_dim] = {}\n",
    "    best_params_d[res_dim]['best_parameters'] = best_parameters\n",
    "    best_params_d[res_dim]['best_values'] = best_values\n",
    "    best_params_d[res_dim]['experiment'] = experiment\n",
    "    # best_params_d[res_dim]['model'] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best parameters\n",
    "\n",
    "with open(os.path.join(RESULTS_PATH, PARAMS_FILE), 'wb') as fh:\n",
    "    pickle.dump(best_params_d, fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load results\n",
    "# with open(os.path.join(RESULTS_PATH, PARAMS_FILE), 'rb') as fh:\n",
    "#     best_params_d = pickle.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{500: {'best_parameters': {'leaking_rate': 0.9040444911438811,\n",
       "   'spectral_radius': 0.20000000000000126,\n",
       "   'input_scaling': 0.10000000000005109,\n",
       "   'bias_scaling': 0.10000000000001198,\n",
       "   'alpha': 0.07885384448822817,\n",
       "   'reservoir_dim': 500},\n",
       "  'best_values': ({'val_accuracy': 90.43231354512487},\n",
       "   {'val_accuracy': {'val_accuracy': 0.010369366007225113}}),\n",
       "  'experiment': SimpleExperiment(None)},\n",
       " 1000: {'best_parameters': {'leaking_rate': 0.7754061698629553,\n",
       "   'spectral_radius': 0.2,\n",
       "   'input_scaling': 0.10000000001325081,\n",
       "   'bias_scaling': 2.1421750283029133,\n",
       "   'alpha': 0.0011090268898778523,\n",
       "   'reservoir_dim': 1000},\n",
       "  'best_values': ({'val_accuracy': 90.97954458186517},\n",
       "   {'val_accuracy': {'val_accuracy': 7.343272431617016e-05}}),\n",
       "  'experiment': SimpleExperiment(None)},\n",
       " 3000: {'best_parameters': {'leaking_rate': 0.5478866628014272,\n",
       "   'spectral_radius': 1.050724183703662,\n",
       "   'input_scaling': 0.5926856626014334,\n",
       "   'bias_scaling': 3.0,\n",
       "   'alpha': 0.305741473239415,\n",
       "   'reservoir_dim': 3000},\n",
       "  'best_values': ({'val_accuracy': 91.37819551039638},\n",
       "   {'val_accuracy': {'val_accuracy': 0.0015544101004215586}}),\n",
       "  'experiment': SimpleExperiment(None)},\n",
       " 5000: {'best_parameters': {'leaking_rate': 0.8870214264847378,\n",
       "   'spectral_radius': 1.4757744372860333,\n",
       "   'input_scaling': 1.8428765970743208,\n",
       "   'bias_scaling': 0.8102592970032985,\n",
       "   'alpha': 0.05578514336395002,\n",
       "   'reservoir_dim': 5000},\n",
       "  'best_values': ({'val_accuracy': 90.60463851192132},\n",
       "   {'val_accuracy': {'val_accuracy': 34.4179091459526}}),\n",
       "  'experiment': SimpleExperiment(None)}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment finished.\n"
     ]
    }
   ],
   "source": [
    "# results\n",
    "\n",
    "results_d = {}\n",
    "\n",
    "for res_dim in [500, 1000, 3000, 5000]:\n",
    "    \n",
    "    best_parameters = best_params_d[res_dim]['best_parameters']\n",
    "    acc, acc_std, time, time_std = wrapped_fitness(best_parameters, return_test_acc=True)\n",
    "    results_d[res_dim] = acc, acc_std, time, time_std\n",
    "    print(\"Experiment finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{500: (90.73946990966797,\n",
       "  0.11632705449085054,\n",
       "  408.56813515666875,\n",
       "  1.1455081803793339),\n",
       " 1000: (91.22894134521485,\n",
       "  0.04894974472903971,\n",
       "  407.6003307029605,\n",
       "  1.0327152541312996),\n",
       " 3000: (91.58420867919922,\n",
       "  0.08221293855280151,\n",
       "  429.60137262716887,\n",
       "  0.9028311085525002),\n",
       " 5000: (91.67894287109375,\n",
       "  0.053025874842887864,\n",
       "  500.0101929595694,\n",
       "  2.9950660369119095)}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(RESULTS_PATH, RESULTS_FILE), 'wb') as fh:\n",
    "    pickle.dump(results_d, fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{500: (90.73946990966797,\n",
       "  0.11632705449085054,\n",
       "  408.56813515666875,\n",
       "  1.1455081803793339),\n",
       " 1000: (91.22894134521485,\n",
       "  0.04894974472903971,\n",
       "  407.6003307029605,\n",
       "  1.0327152541312996),\n",
       " 3000: (91.58420867919922,\n",
       "  0.08221293855280151,\n",
       "  429.60137262716887,\n",
       "  0.9028311085525002),\n",
       " 5000: (91.67894287109375,\n",
       "  0.053025874842887864,\n",
       "  500.0101929595694,\n",
       "  2.9950660369119095)}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load results\n",
    "# with open(os.path.join(RESULTS_PATH, RESULTS_FILE), 'rb') as fh:\n",
    "#     results_d = pickle.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{500: (90.10525970458984,\n",
       "  0.10945550684716364,\n",
       "  297.90255975187756,\n",
       "  3.3313685829787247),\n",
       " 1000: (90.73420715332031,\n",
       "  0.17562757065845663,\n",
       "  297.3748939599842,\n",
       "  4.823391432702744),\n",
       " 3000: (91.11578521728515,\n",
       "  0.13959970852840675,\n",
       "  298.7870469949674,\n",
       "  4.94682598925355),\n",
       " 5000: (91.30262756347656,\n",
       "  0.1755489999248224,\n",
       "  304.08307738858275,\n",
       "  5.16690769692509)}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load results\n",
    "with open(os.path.join(RESULTS_PATH, 'AGnews-4_results.pkl'), 'rb') as fh:\n",
    "    results = pickle.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{500: (90.10525970458984,\n",
       "  0.10945550684716364,\n",
       "  297.90255975187756,\n",
       "  3.3313685829787247),\n",
       " 1000: (90.73420715332031,\n",
       "  0.17562757065845663,\n",
       "  297.3748939599842,\n",
       "  4.823391432702744),\n",
       " 3000: (91.11578521728515,\n",
       "  0.13959970852840675,\n",
       "  298.7870469949674,\n",
       "  4.94682598925355),\n",
       " 5000: (91.30262756347656,\n",
       "  0.1755489999248224,\n",
       "  304.08307738858275,\n",
       "  5.16690769692509)}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Results unidirectional\n",
    "\n",
    "{500: (90.10525970458984,\n",
    "  0.10945550684716364,\n",
    "  297.90255975187756,\n",
    "  3.3313685829787247),\n",
    " 1000: (90.73420715332031,\n",
    "  0.17562757065845663,\n",
    "  297.3748939599842,\n",
    "  4.823391432702744),\n",
    " 3000: (91.11578521728515,\n",
    "  0.13959970852840675,\n",
    "  298.7870469949674,\n",
    "  4.94682598925355),\n",
    " 5000: (91.30262756347656,\n",
    "  0.1755489999248224,\n",
    "  304.08307738858275,\n",
    "  5.16690769692509)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Results bidirectional\n",
    "\n",
    "{500: (90.73946990966797,\n",
    "  0.11632705449085054,\n",
    "  408.56813515666875,\n",
    "  1.1455081803793339),\n",
    " 1000: (91.22894134521485,\n",
    "  0.04894974472903971,\n",
    "  407.6003307029605,\n",
    "  1.0327152541312996),\n",
    " 3000: (91.58420867919922,\n",
    "  0.08221293855280151,\n",
    "  429.60137262716887,\n",
    "  0.9028311085525002),\n",
    " 5000: (91.67894287109375,\n",
    "  0.053025874842887864,\n",
    "  500.0101929595694,\n",
    "  2.9950660369119095)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
