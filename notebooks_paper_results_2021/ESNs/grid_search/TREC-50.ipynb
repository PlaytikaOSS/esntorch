{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TREC-50: Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librairies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Need ``ax-platform==0.1.20``\n",
    "\n",
    "Install them from command line if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ipywidgets\n",
    "# !jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath(\"../../..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "# from tqdm import tqdm\n",
    "# from tqdm import tqdm_notebook\n",
    "# from tqdm import tqdm_notebook as tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import BertModel\n",
    "from transformers.data.data_collator import DataCollatorWithPadding\n",
    "\n",
    "import esntorch.core.reservoir as res\n",
    "import esntorch.core.learning_algo as la\n",
    "import esntorch.core.merging_strategy as ms\n",
    "import esntorch.core.esn as esn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 4234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_PATH = '/raid/home/jeremiec/Ax_results/ESN_v2' # path of your result folder\n",
    "CACHE_DIR = '/raid/home/jeremiec/huggingface_datasets' # path of your folder\n",
    "RESULTS_FILE = 'trec-50_params.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename correct column as 'labels': depends on the dataset you load\n",
    "\n",
    "def tokenize(sample):\n",
    "    \"\"\"Tokenize sample\"\"\"\n",
    "    \n",
    "    sample = tokenizer(sample['text'], truncation=True, padding=False, return_length=True)\n",
    "    \n",
    "    return sample\n",
    "    \n",
    "def load_and_enrich_dataset(dataset_name, split, cache_dir):\n",
    "    \"\"\"\n",
    "    Load dataset from the datasets library of HuggingFace.\n",
    "    Tokenize and add length.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load dataset\n",
    "    dataset = load_dataset(dataset_name, split=split, cache_dir=CACHE_DIR)\n",
    "    \n",
    "    # Rename label column for tokenization purposes (use 'label-fine' for fine-grained labels)\n",
    "    dataset = dataset.rename_column('label-fine', 'labels')\n",
    "    \n",
    "    # Tokenize data\n",
    "    dataset = dataset.map(tokenize, batched=True)\n",
    "    dataset = dataset.rename_column('length', 'lengths')\n",
    "    dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels', 'lengths'])\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset trec (/raid/home/jeremiec/huggingface_datasets/trec/default/1.1.0/751da1ab101b8d297a3d6e9c79ee9b0173ff94c4497b75677b59b61d5467a9b9)\n",
      "Loading cached processed dataset at /raid/home/jeremiec/huggingface_datasets/trec/default/1.1.0/751da1ab101b8d297a3d6e9c79ee9b0173ff94c4497b75677b59b61d5467a9b9/cache-560ad7392e08dc9c.arrow\n",
      "Loading cached split indices for dataset at /raid/home/jeremiec/huggingface_datasets/trec/default/1.1.0/751da1ab101b8d297a3d6e9c79ee9b0173ff94c4497b75677b59b61d5467a9b9/cache-964c55c0774369bb.arrow and /raid/home/jeremiec/huggingface_datasets/trec/default/1.1.0/751da1ab101b8d297a3d6e9c79ee9b0173ff94c4497b75677b59b61d5467a9b9/cache-b4b0dba39bdc7546.arrow\n",
      "Using custom data configuration default\n",
      "Reusing dataset trec (/raid/home/jeremiec/huggingface_datasets/trec/default/1.1.0/751da1ab101b8d297a3d6e9c79ee9b0173ff94c4497b75677b59b61d5467a9b9)\n",
      "Loading cached processed dataset at /raid/home/jeremiec/huggingface_datasets/trec/default/1.1.0/751da1ab101b8d297a3d6e9c79ee9b0173ff94c4497b75677b59b61d5467a9b9/cache-2f6894ef503b7225.arrow\n",
      "Loading cached sorted indices for dataset at /raid/home/jeremiec/huggingface_datasets/trec/default/1.1.0/751da1ab101b8d297a3d6e9c79ee9b0173ff94c4497b75677b59b61d5467a9b9/cache-f4d0aab95b1af03d.arrow\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "full_train_dataset = load_and_enrich_dataset('trec', split='train', cache_dir=CACHE_DIR)\n",
    "\n",
    "# Select 30% of the dataset # 30% only here, since too many labels, mismatches, etc.\n",
    "full_train_dataset = full_train_dataset.train_test_split(train_size=0.3, shuffle=True)['train']\n",
    "\n",
    "# Build mini train and val sets\n",
    "train_val_datasets = full_train_dataset.train_test_split(train_size=0.8, shuffle=True)\n",
    "train_dataset = train_val_datasets['train'].sort(\"lengths\")\n",
    "val_dataset = train_val_datasets['test'].sort(\"lengths\")\n",
    "\n",
    "test_dataset = load_and_enrich_dataset('trec', split='test', cache_dir=CACHE_DIR).sort(\"lengths\")\n",
    "\n",
    "dataset_d = {\n",
    "    'full_train': full_train_dataset,\n",
    "    'train': train_dataset,\n",
    "    'val': val_dataset,\n",
    "    'test': test_dataset\n",
    "    }\n",
    "\n",
    "dataloader_d = {}\n",
    "for k, v in dataset_d.items():\n",
    "    dataloader_d[k] = torch.utils.data.DataLoader(v, batch_size=256, collate_fn=DataCollatorWithPadding(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'full_train': Dataset({\n",
       "     features: ['attention_mask', 'input_ids', 'label-coarse', 'labels', 'lengths', 'text', 'token_type_ids'],\n",
       "     num_rows: 1635\n",
       " }),\n",
       " 'train': Dataset({\n",
       "     features: ['attention_mask', 'input_ids', 'label-coarse', 'labels', 'lengths', 'text', 'token_type_ids'],\n",
       "     num_rows: 1308\n",
       " }),\n",
       " 'val': Dataset({\n",
       "     features: ['attention_mask', 'input_ids', 'label-coarse', 'labels', 'lengths', 'text', 'token_type_ids'],\n",
       "     num_rows: 327\n",
       " }),\n",
       " 'test': Dataset({\n",
       "     features: ['attention_mask', 'input_ids', 'label-coarse', 'labels', 'lengths', 'text', 'token_type_ids'],\n",
       "     num_rows: 500\n",
       " })}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set(dataset_d['val']['labels'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_d = {\n",
    "            'reservoir_dim' : [1000], # we fix the reservoir dim\n",
    "            'spectral_radius' : [0.5, 1.0, 1.5],\n",
    "            'leaking_rate': [0.1, 0.5, 0.9],\n",
    "            'input_scaling' : [0.1, 1.0], \n",
    "            'bias_scaling' : [0.0],\n",
    "            'sparsity' : [0.0, 0.99],\n",
    "            'activation_function' : ['tanh', 'relu'],\n",
    "            'alpha' : [0.1, 1.0, 10.0]\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'reservoir_dim': 1000,\n",
       "  'spectral_radius': 0.5,\n",
       "  'leaking_rate': 0.1,\n",
       "  'input_scaling': 0.1,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.0,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 0.1},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 0.5,\n",
       "  'leaking_rate': 0.1,\n",
       "  'input_scaling': 0.1,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.0,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 1.0},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 0.5,\n",
       "  'leaking_rate': 0.1,\n",
       "  'input_scaling': 0.1,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.0,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 10.0},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 0.5,\n",
       "  'leaking_rate': 0.1,\n",
       "  'input_scaling': 0.1,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.99,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 0.1},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 0.5,\n",
       "  'leaking_rate': 0.1,\n",
       "  'input_scaling': 0.1,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.99,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 1.0},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 0.5,\n",
       "  'leaking_rate': 0.1,\n",
       "  'input_scaling': 0.1,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.99,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 10.0},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 0.5,\n",
       "  'leaking_rate': 0.1,\n",
       "  'input_scaling': 1.0,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.0,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 0.1},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 0.5,\n",
       "  'leaking_rate': 0.1,\n",
       "  'input_scaling': 1.0,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.0,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 1.0},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 0.5,\n",
       "  'leaking_rate': 0.1,\n",
       "  'input_scaling': 1.0,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.0,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 10.0},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 0.5,\n",
       "  'leaking_rate': 0.1,\n",
       "  'input_scaling': 1.0,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.99,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 0.1},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 0.5,\n",
       "  'leaking_rate': 0.1,\n",
       "  'input_scaling': 1.0,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.99,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 1.0},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 0.5,\n",
       "  'leaking_rate': 0.1,\n",
       "  'input_scaling': 1.0,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.99,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 10.0},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 0.5,\n",
       "  'leaking_rate': 0.5,\n",
       "  'input_scaling': 0.1,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.0,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 0.1},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 0.5,\n",
       "  'leaking_rate': 0.5,\n",
       "  'input_scaling': 0.1,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.0,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 1.0},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 0.5,\n",
       "  'leaking_rate': 0.5,\n",
       "  'input_scaling': 0.1,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.0,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 10.0},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 0.5,\n",
       "  'leaking_rate': 0.5,\n",
       "  'input_scaling': 0.1,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.99,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 0.1},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 0.5,\n",
       "  'leaking_rate': 0.5,\n",
       "  'input_scaling': 0.1,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.99,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 1.0},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 0.5,\n",
       "  'leaking_rate': 0.5,\n",
       "  'input_scaling': 0.1,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.99,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 10.0},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 0.5,\n",
       "  'leaking_rate': 0.5,\n",
       "  'input_scaling': 1.0,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.0,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 0.1},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 0.5,\n",
       "  'leaking_rate': 0.5,\n",
       "  'input_scaling': 1.0,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.0,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 1.0},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 0.5,\n",
       "  'leaking_rate': 0.5,\n",
       "  'input_scaling': 1.0,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.0,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 10.0},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 0.5,\n",
       "  'leaking_rate': 0.5,\n",
       "  'input_scaling': 1.0,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.99,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 0.1},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 0.5,\n",
       "  'leaking_rate': 0.5,\n",
       "  'input_scaling': 1.0,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.99,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 1.0},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 0.5,\n",
       "  'leaking_rate': 0.5,\n",
       "  'input_scaling': 1.0,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.99,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 10.0},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 0.5,\n",
       "  'leaking_rate': 0.9,\n",
       "  'input_scaling': 0.1,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.0,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 0.1},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 0.5,\n",
       "  'leaking_rate': 0.9,\n",
       "  'input_scaling': 0.1,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.0,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 1.0},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 0.5,\n",
       "  'leaking_rate': 0.9,\n",
       "  'input_scaling': 0.1,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.0,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 10.0},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 0.5,\n",
       "  'leaking_rate': 0.9,\n",
       "  'input_scaling': 0.1,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.99,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 0.1},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 0.5,\n",
       "  'leaking_rate': 0.9,\n",
       "  'input_scaling': 0.1,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.99,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 1.0},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 0.5,\n",
       "  'leaking_rate': 0.9,\n",
       "  'input_scaling': 0.1,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.99,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 10.0},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 0.5,\n",
       "  'leaking_rate': 0.9,\n",
       "  'input_scaling': 1.0,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.0,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 0.1},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 0.5,\n",
       "  'leaking_rate': 0.9,\n",
       "  'input_scaling': 1.0,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.0,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 1.0},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 0.5,\n",
       "  'leaking_rate': 0.9,\n",
       "  'input_scaling': 1.0,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.0,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 10.0},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 0.5,\n",
       "  'leaking_rate': 0.9,\n",
       "  'input_scaling': 1.0,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.99,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 0.1},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 0.5,\n",
       "  'leaking_rate': 0.9,\n",
       "  'input_scaling': 1.0,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.99,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 1.0},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 0.5,\n",
       "  'leaking_rate': 0.9,\n",
       "  'input_scaling': 1.0,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.99,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 10.0},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 1.0,\n",
       "  'leaking_rate': 0.1,\n",
       "  'input_scaling': 0.1,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.0,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 0.1},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 1.0,\n",
       "  'leaking_rate': 0.1,\n",
       "  'input_scaling': 0.1,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.0,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 1.0},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 1.0,\n",
       "  'leaking_rate': 0.1,\n",
       "  'input_scaling': 0.1,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.0,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 10.0},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 1.0,\n",
       "  'leaking_rate': 0.1,\n",
       "  'input_scaling': 0.1,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.99,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 0.1},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 1.0,\n",
       "  'leaking_rate': 0.1,\n",
       "  'input_scaling': 0.1,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.99,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 1.0},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 1.0,\n",
       "  'leaking_rate': 0.1,\n",
       "  'input_scaling': 0.1,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.99,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 10.0},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 1.0,\n",
       "  'leaking_rate': 0.1,\n",
       "  'input_scaling': 1.0,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.0,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 0.1},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 1.0,\n",
       "  'leaking_rate': 0.1,\n",
       "  'input_scaling': 1.0,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.0,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 1.0},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 1.0,\n",
       "  'leaking_rate': 0.1,\n",
       "  'input_scaling': 1.0,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.0,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 10.0},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 1.0,\n",
       "  'leaking_rate': 0.1,\n",
       "  'input_scaling': 1.0,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.99,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 0.1},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 1.0,\n",
       "  'leaking_rate': 0.1,\n",
       "  'input_scaling': 1.0,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.99,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 1.0},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 1.0,\n",
       "  'leaking_rate': 0.1,\n",
       "  'input_scaling': 1.0,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.99,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 10.0},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 1.0,\n",
       "  'leaking_rate': 0.5,\n",
       "  'input_scaling': 0.1,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.0,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 0.1},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 1.0,\n",
       "  'leaking_rate': 0.5,\n",
       "  'input_scaling': 0.1,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.0,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 1.0},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 1.0,\n",
       "  'leaking_rate': 0.5,\n",
       "  'input_scaling': 0.1,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.0,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 10.0},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 1.0,\n",
       "  'leaking_rate': 0.5,\n",
       "  'input_scaling': 0.1,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.99,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 0.1},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 1.0,\n",
       "  'leaking_rate': 0.5,\n",
       "  'input_scaling': 0.1,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.99,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 1.0},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 1.0,\n",
       "  'leaking_rate': 0.5,\n",
       "  'input_scaling': 0.1,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.99,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 10.0},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 1.0,\n",
       "  'leaking_rate': 0.5,\n",
       "  'input_scaling': 1.0,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.0,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 0.1},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 1.0,\n",
       "  'leaking_rate': 0.5,\n",
       "  'input_scaling': 1.0,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.0,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 1.0},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 1.0,\n",
       "  'leaking_rate': 0.5,\n",
       "  'input_scaling': 1.0,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.0,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 10.0},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 1.0,\n",
       "  'leaking_rate': 0.5,\n",
       "  'input_scaling': 1.0,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.99,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 0.1},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 1.0,\n",
       "  'leaking_rate': 0.5,\n",
       "  'input_scaling': 1.0,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.99,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 1.0},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 1.0,\n",
       "  'leaking_rate': 0.5,\n",
       "  'input_scaling': 1.0,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.99,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 10.0},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 1.0,\n",
       "  'leaking_rate': 0.9,\n",
       "  'input_scaling': 0.1,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.0,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 0.1},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 1.0,\n",
       "  'leaking_rate': 0.9,\n",
       "  'input_scaling': 0.1,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.0,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 1.0},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 1.0,\n",
       "  'leaking_rate': 0.9,\n",
       "  'input_scaling': 0.1,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.0,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 10.0},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 1.0,\n",
       "  'leaking_rate': 0.9,\n",
       "  'input_scaling': 0.1,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.99,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 0.1},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 1.0,\n",
       "  'leaking_rate': 0.9,\n",
       "  'input_scaling': 0.1,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.99,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 1.0},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 1.0,\n",
       "  'leaking_rate': 0.9,\n",
       "  'input_scaling': 0.1,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.99,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 10.0},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 1.0,\n",
       "  'leaking_rate': 0.9,\n",
       "  'input_scaling': 1.0,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.0,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 0.1},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 1.0,\n",
       "  'leaking_rate': 0.9,\n",
       "  'input_scaling': 1.0,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.0,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 1.0},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 1.0,\n",
       "  'leaking_rate': 0.9,\n",
       "  'input_scaling': 1.0,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.0,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 10.0},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 1.0,\n",
       "  'leaking_rate': 0.9,\n",
       "  'input_scaling': 1.0,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.99,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 0.1},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 1.0,\n",
       "  'leaking_rate': 0.9,\n",
       "  'input_scaling': 1.0,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.99,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 1.0},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 1.0,\n",
       "  'leaking_rate': 0.9,\n",
       "  'input_scaling': 1.0,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.99,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 10.0},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 1.5,\n",
       "  'leaking_rate': 0.1,\n",
       "  'input_scaling': 0.1,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.0,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 0.1},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 1.5,\n",
       "  'leaking_rate': 0.1,\n",
       "  'input_scaling': 0.1,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.0,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 1.0},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 1.5,\n",
       "  'leaking_rate': 0.1,\n",
       "  'input_scaling': 0.1,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.0,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 10.0},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 1.5,\n",
       "  'leaking_rate': 0.1,\n",
       "  'input_scaling': 0.1,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.99,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 0.1},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 1.5,\n",
       "  'leaking_rate': 0.1,\n",
       "  'input_scaling': 0.1,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.99,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 1.0},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 1.5,\n",
       "  'leaking_rate': 0.1,\n",
       "  'input_scaling': 0.1,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.99,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 10.0},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 1.5,\n",
       "  'leaking_rate': 0.1,\n",
       "  'input_scaling': 1.0,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.0,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 0.1},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 1.5,\n",
       "  'leaking_rate': 0.1,\n",
       "  'input_scaling': 1.0,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.0,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 1.0},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 1.5,\n",
       "  'leaking_rate': 0.1,\n",
       "  'input_scaling': 1.0,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.0,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 10.0},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 1.5,\n",
       "  'leaking_rate': 0.1,\n",
       "  'input_scaling': 1.0,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.99,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 0.1},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 1.5,\n",
       "  'leaking_rate': 0.1,\n",
       "  'input_scaling': 1.0,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.99,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 1.0},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 1.5,\n",
       "  'leaking_rate': 0.1,\n",
       "  'input_scaling': 1.0,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.99,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 10.0},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 1.5,\n",
       "  'leaking_rate': 0.5,\n",
       "  'input_scaling': 0.1,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.0,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 0.1},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 1.5,\n",
       "  'leaking_rate': 0.5,\n",
       "  'input_scaling': 0.1,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.0,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 1.0},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 1.5,\n",
       "  'leaking_rate': 0.5,\n",
       "  'input_scaling': 0.1,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.0,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 10.0},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 1.5,\n",
       "  'leaking_rate': 0.5,\n",
       "  'input_scaling': 0.1,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.99,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 0.1},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 1.5,\n",
       "  'leaking_rate': 0.5,\n",
       "  'input_scaling': 0.1,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.99,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 1.0},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 1.5,\n",
       "  'leaking_rate': 0.5,\n",
       "  'input_scaling': 0.1,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.99,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 10.0},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 1.5,\n",
       "  'leaking_rate': 0.5,\n",
       "  'input_scaling': 1.0,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.0,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 0.1},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 1.5,\n",
       "  'leaking_rate': 0.5,\n",
       "  'input_scaling': 1.0,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.0,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 1.0},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 1.5,\n",
       "  'leaking_rate': 0.5,\n",
       "  'input_scaling': 1.0,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.0,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 10.0},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 1.5,\n",
       "  'leaking_rate': 0.5,\n",
       "  'input_scaling': 1.0,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.99,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 0.1},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 1.5,\n",
       "  'leaking_rate': 0.5,\n",
       "  'input_scaling': 1.0,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.99,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 1.0},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 1.5,\n",
       "  'leaking_rate': 0.5,\n",
       "  'input_scaling': 1.0,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.99,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 10.0},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 1.5,\n",
       "  'leaking_rate': 0.9,\n",
       "  'input_scaling': 0.1,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.0,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 0.1},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 1.5,\n",
       "  'leaking_rate': 0.9,\n",
       "  'input_scaling': 0.1,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.0,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 1.0},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 1.5,\n",
       "  'leaking_rate': 0.9,\n",
       "  'input_scaling': 0.1,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.0,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 10.0},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 1.5,\n",
       "  'leaking_rate': 0.9,\n",
       "  'input_scaling': 0.1,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.99,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 0.1},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 1.5,\n",
       "  'leaking_rate': 0.9,\n",
       "  'input_scaling': 0.1,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.99,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 1.0},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 1.5,\n",
       "  'leaking_rate': 0.9,\n",
       "  'input_scaling': 0.1,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.99,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 10.0},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 1.5,\n",
       "  'leaking_rate': 0.9,\n",
       "  'input_scaling': 1.0,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.0,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 0.1},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 1.5,\n",
       "  'leaking_rate': 0.9,\n",
       "  'input_scaling': 1.0,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.0,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 1.0},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 1.5,\n",
       "  'leaking_rate': 0.9,\n",
       "  'input_scaling': 1.0,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.0,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 10.0},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 1.5,\n",
       "  'leaking_rate': 0.9,\n",
       "  'input_scaling': 1.0,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.99,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 0.1},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 1.5,\n",
       "  'leaking_rate': 0.9,\n",
       "  'input_scaling': 1.0,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.99,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 1.0},\n",
       " {'reservoir_dim': 1000,\n",
       "  'spectral_radius': 1.5,\n",
       "  'leaking_rate': 0.9,\n",
       "  'input_scaling': 1.0,\n",
       "  'bias_scaling': 0.0,\n",
       "  'sparsity': 0.99,\n",
       "  'activation_function': 'relu',\n",
       "  'alpha': 10.0}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_l = [ dict(zip(params_d, v)) for v in product(*params_d.values()) ]\n",
    "params_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(params_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We store the results as follows:\n",
    "# esn_results_l = \n",
    "# [\n",
    "# {params_1 : ... , acc_1 : [...] , times_1 : [...]},\n",
    "# {params_2 : ... , acc_2 : [...] , times_2 : [...]},\n",
    "# {params_3 : ... , acc_3 : [...] , times_3 : [...]},\n",
    "# ...\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b73976859b754f4fade8d2ef7718fac6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c040eaeebad54ccb9eb85ad10913f24d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model downloaded: bert-base-uncased\n",
      "training time: 1.0910586034879088\n",
      "accuracy: 63.30275344848633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model downloaded: bert-base-uncased\n",
      "training time: 1.0699767665937543\n",
      "accuracy: 63.91437530517578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model downloaded: bert-base-uncased\n",
      "training time: 1.023872290737927\n",
      "accuracy: 62.996944427490234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model downloaded: bert-base-uncased\n",
      "training time: 1.1723271869122982\n",
      "accuracy: 64.52599334716797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model downloaded: bert-base-uncased\n",
      "training time: 0.9905601432546973\n",
      "accuracy: 66.05504608154297\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7efa0a32c69640b5a883700f5a551a59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model downloaded: bert-base-uncased\n",
      "training time: 0.9392038267105818\n",
      "accuracy: 69.41896057128906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model downloaded: bert-base-uncased\n",
      "training time: 1.0732659827917814\n",
      "accuracy: 67.88990783691406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model downloaded: bert-base-uncased\n",
      "training time: 1.069861431606114\n",
      "accuracy: 69.72476959228516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model downloaded: bert-base-uncased\n",
      "training time: 0.9851036602631211\n",
      "accuracy: 67.88990783691406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model downloaded: bert-base-uncased\n",
      "training time: 0.9886262137442827\n",
      "accuracy: 68.80734252929688\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d63612cd4aed4f51b39469ead739ee18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model downloaded: bert-base-uncased\n",
      "training time: 1.1162245888262987\n",
      "accuracy: 66.66667175292969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model downloaded: bert-base-uncased\n",
      "training time: 0.9891398139297962\n",
      "accuracy: 66.66667175292969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model downloaded: bert-base-uncased\n",
      "training time: 1.0512135354802012\n",
      "accuracy: 67.27828979492188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model downloaded: bert-base-uncased\n",
      "training time: 1.0315750958397985\n",
      "accuracy: 66.36085510253906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model downloaded: bert-base-uncased\n",
      "training time: 1.0116072725504637\n",
      "accuracy: 65.74923706054688\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c86d2a26b2464c46a5c8df02ea10eafc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model downloaded: bert-base-uncased\n",
      "training time: 0.9947473583742976\n",
      "accuracy: 65.13761901855469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model downloaded: bert-base-uncased\n",
      "training time: 1.0251554166898131\n",
      "accuracy: 62.38532257080078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model downloaded: bert-base-uncased\n",
      "training time: 1.1028260784223676\n",
      "accuracy: 61.16208267211914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model downloaded: bert-base-uncased\n",
      "training time: 1.0888839391991496\n",
      "accuracy: 60.55046081542969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model downloaded: bert-base-uncased\n",
      "training time: 0.983046249486506\n",
      "accuracy: 64.8318099975586\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7521d880e6047628151b9ba0ef14638",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model downloaded: bert-base-uncased\n",
      "training time: 0.988458413630724\n",
      "accuracy: 69.72476959228516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model downloaded: bert-base-uncased\n",
      "training time: 0.9590783901512623\n",
      "accuracy: 67.58409881591797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "esn_results_l = []\n",
    "# cbs_results_l = []\n",
    "\n",
    "# loop over params\n",
    "for params in tqdm(params_l):\n",
    "\n",
    "    esn_d = {}\n",
    "    esn_d['params'] = params\n",
    "    esn_d['accuracy'] = []\n",
    "    esn_d['time'] = []\n",
    "\n",
    "#     cbs_d = {}\n",
    "#     cbs_d['params'] = params\n",
    "#     cbs_d['accuracy'] = []\n",
    "#     cbs_d['time'] = []\n",
    "\n",
    "    # loop over seeds\n",
    "    for seed in tqdm([42, 127, 74684, 888, 7716843]):\n",
    "\n",
    "        torch.manual_seed(seed)\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "\n",
    "#         # loop over modes...\n",
    "#         for mode in tqdm(['esn', 'linear_layer']):\n",
    "\n",
    "        # ESN parameters\n",
    "        esn_params = {\n",
    "                    'embedding_weights': 'bert-base-uncased', # TEXT.vocab.vectors,\n",
    "                    'distribution' : 'gaussian',              # uniform, gaussian\n",
    "                    'input_dim' : 768,                        # dim of BERT encoding!\n",
    "                    'reservoir_dim' : None,\n",
    "                    'bias_scaling' : None,\n",
    "                    'sparsity' : None,\n",
    "                    'spectral_radius' : None,\n",
    "                    'leaking_rate': None,\n",
    "                    'activation_function' : None,\n",
    "                    'input_scaling' : None,\n",
    "                    'mean' : 0.0,\n",
    "                    'std' : 1.0,\n",
    "                    'merging_strategy' : 'mean',\n",
    "                    'bidirectional' : False,\n",
    "                    'device' : device,\n",
    "                    'mode' : 'esn', #None,\n",
    "                    'seed' : seed\n",
    "                     }\n",
    "\n",
    "        # Fill up the reservoir\n",
    "        for k, v in params.items():\n",
    "            if k in esn_params.keys():\n",
    "                esn_params[k] = v\n",
    "#         esn_params['mode'] = mode\n",
    "\n",
    "        # Instantiate the ESN\n",
    "        ESN = esn.EchoStateNetwork(**esn_params)\n",
    "        ESN.learning_algo = la.RidgeRegression(alpha=params['alpha'])\n",
    "        ESN = ESN.to(device)\n",
    "\n",
    "        # Warm up the ESN\n",
    "        nb_sentences = 10\n",
    "\n",
    "        for i in range(nb_sentences): \n",
    "            sentence = dataset_d[\"train\"].select([i])\n",
    "            dataloader_tmp = torch.utils.data.DataLoader(sentence, \n",
    "                                                         batch_size=1, \n",
    "                                                         collate_fn=DataCollatorWithPadding(tokenizer))  \n",
    "\n",
    "            for sentence in dataloader_tmp:\n",
    "                ESN.warm_up(sentence)\n",
    "\n",
    "        # train on mini train set\n",
    "        t0 = timer()\n",
    "        ESN.fit(dataloader_d[\"train\"])\n",
    "        t1 = timer()\n",
    "        time = t1 - t0\n",
    "        print(f'training time: {time}')\n",
    "\n",
    "        # acc on mini validation set\n",
    "        val_pred, val_acc = ESN.predict(dataloader_d[\"val\"], verbose=False)\n",
    "        print(f'accuracy: {val_acc.item()}')\n",
    "\n",
    "        # save results\n",
    "        if esn_params['mode'] == 'esn':\n",
    "            esn_d['time'].append(time)\n",
    "            esn_d['accuracy'].append(val_acc.item())\n",
    "            # print(esn_d)\n",
    "\n",
    "#         elif mode == 'linear_layer':\n",
    "#             cbs_d['time'].append(time)\n",
    "#             cbs_d['accuracy'].append(val_acc.item())\n",
    "\n",
    "        # clean objects\n",
    "        del ESN\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    esn_results_l.append(esn_d)\n",
    "#     cbs_results_l.append(cbs_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# esn_results_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in esn_results_l:\n",
    "    d['accuracy_mean'] = np.mean(d['accuracy'])\n",
    "    d['accuracy_std'] = np.std(d['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(RESULTS_PATH, RESULTS_FILE), 'wb') as fh:\n",
    "    pickle.dump(esn_results_l, fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(os.path.join(RESULTS_PATH, RESULTS_FILE), 'rb') as fh:\n",
    "#     esn_results_l = pickle.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "esn_results_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
