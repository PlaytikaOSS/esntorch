{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ESNs for Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides a use case example of the ``EsnTorch`` library. It describes the implementation of an **Echo State Network (ESN)** for text classification on the **TREC-6** dataset (question classification).\n",
    "\n",
    "The instantiation, training and evaluation of an ESN for text classification is achieved via the following steps:\n",
    "1. Import libraries and modules\n",
    "2. Load and prepare data\n",
    "3. Instantiate the model:\n",
    "    - specify parameters\n",
    "    - specify learning algorithm\n",
    "    - warm up\n",
    "4. Train\n",
    "5. Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers==4.8.2\n",
    "# !pip install datasets==1.7.0\n",
    "# !pip install tqdm\n",
    "# !pip install torch==1.7.1+cu110 torchvision==0.8.2+cu110 torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To enable progress bars in jupyter:\n",
    "\n",
    "# pip install ipywidgets\n",
    "# jupyter nbextension enable --py widgetsnbextension\n",
    "# conda install -c conda-forge nodejs\n",
    "# jupyter labextension install @jupyter-widgets/jupyterlab-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment this if library is installed!\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath(\"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm_notebook\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import torch\n",
    "\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers.data.data_collator import DataCollatorWithPadding\n",
    "\n",
    "import esntorch.core.reservoir as res\n",
    "import esntorch.core.learning_algo as la\n",
    "import esntorch.core.merging_strategy as ms\n",
    "import esntorch.core.esn as esn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set device (cpu or gpu if available)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and tokenize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom functions for loading and preparing data\n",
    "def tokenize(sample):\n",
    "    \"\"\"Tokenize sample\"\"\"\n",
    "    \n",
    "    sample = tokenizer(sample['text'], truncation=True, padding=False, return_length=True)\n",
    "    \n",
    "    return sample\n",
    "    \n",
    "def load_and_prepare_dataset(dataset_name, split, cache_dir):\n",
    "    \"\"\"\n",
    "    Load dataset from the datasets library of HuggingFace.\n",
    "    Tokenize and add length.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load dataset\n",
    "    dataset = load_dataset(dataset_name, split=split, cache_dir=CACHE_DIR)\n",
    "    \n",
    "    # Rename label column for tokenization purposes (use 'label-fine' for fine-grained labels)\n",
    "    dataset = dataset.rename_column('label-coarse', 'labels')\n",
    "    \n",
    "    # Tokenize data\n",
    "    dataset = dataset.map(tokenize, batched=True)\n",
    "    dataset = dataset.rename_column('length', 'lengths')\n",
    "    dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels', 'lengths'])\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset trec (cache_dir/trec/default/1.1.0/751da1ab101b8d297a3d6e9c79ee9b0173ff94c4497b75677b59b61d5467a9b9)\n",
      "Parameter 'function'=<function tokenize at 0x7f0f3f218e50> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee7e23074c9c4054b3fad2e25e9a78ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "272d6e85c1ea4eb6b483607eb17d5a8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load BERT tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load and prepare data\n",
    "CACHE_DIR = 'cache_dir/' # put your path here\n",
    "\n",
    "full_dataset = load_and_prepare_dataset('trec', split=None, cache_dir=CACHE_DIR)\n",
    "train_dataset = full_dataset['train'].sort(\"lengths\")\n",
    "test_dataset = full_dataset['test'].sort(\"lengths\")\n",
    "\n",
    "# Create dict of all datasets\n",
    "dataset_d = {\n",
    "    'train': train_dataset,\n",
    "    'test': test_dataset\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': Dataset({\n",
       "     features: ['attention_mask', 'input_ids', 'label-fine', 'labels', 'lengths', 'text', 'token_type_ids'],\n",
       "     num_rows: 5452\n",
       " }),\n",
       " 'test': Dataset({\n",
       "     features: ['attention_mask', 'input_ids', 'label-fine', 'labels', 'lengths', 'text', 'token_type_ids'],\n",
       "     num_rows: 500\n",
       " })}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dict of dataloaders\n",
    "\n",
    "dataloader_d = {}\n",
    "\n",
    "for k, v in dataset_d.items():\n",
    "    dataloader_d[k] = torch.utils.data.DataLoader(v, batch_size=256, \n",
    "                                                  collate_fn=DataCollatorWithPadding(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': <torch.utils.data.dataloader.DataLoader at 0x7f0f3f10da30>,\n",
       " 'test': <torch.utils.data.dataloader.DataLoader at 0x7f0f300267c0>}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instanciate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ESN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most parameters are self-explanatory for a reader familiar with ESNs. Please refer to the documentation for further details. \n",
    "\n",
    "The ``mode`` parameter represents the type of reservoir to be considered:\n",
    "- ``recurrent_layer``: implements a **classical recurrent reservoir**, specified among others by its ``dim``, ``sparsity``, ``spectral_radius``, ``leaking_rate`` and ``activation_function``.\n",
    "- ``linear_layer``: implements a simple **linear layer** specified by its ``dim`` and ``activation_function``.\n",
    "- ``no_layer``: implements **the absence of reservoir**, meaning that the embedded inputs are directly fed to the the learning algorithms.\n",
    "\n",
    "The comparison between the ``recurrent_layer`` and the ``no_layer``modes enables to assess the impact of the reservoir on the results. The comparison between the ``recurrent_layer`` and the ``linear_layer``modes allows to assess the importance of the recurrence of the reservoir on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model downloaded: bert-base-uncased\n",
      "dim and input_dim 1000 768\n"
     ]
    }
   ],
   "source": [
    "# ESN parameters\n",
    "esn_params = {\n",
    "            'embedding_weights': 'bert-base-uncased', # name of Hugging Face model\n",
    "            'dim': 1000,\n",
    "            'sparsity': 0.9,\n",
    "            'spectral_radius': 0.9,\n",
    "            'leaking_rate': 0.5,\n",
    "            'activation_function': 'tanh', # 'tanh', 'relu'\n",
    "            'bias_scaling': 0.1,\n",
    "            'input_scaling': 0.1,\n",
    "            'mean': 0.0,\n",
    "            'std': 1.0,     \n",
    "            'learning_algo': None,         # initialzed below\n",
    "            'criterion': None,             # initialzed below (only for learning algos trained with SGD)\n",
    "            'optimizer': None,             # initialzed below (only for learning algos trained with SGD)\n",
    "            'merging_strategy': None,    # 'mean', 'last', None\n",
    "            'bidirectional': True,        # True, False\n",
    "            'mode' : 'recurrent_layer',    # 'no_layer', 'linear_layer', 'recurrent_layer'\n",
    "            'device': device,  \n",
    "            'seed': 42\n",
    "            }\n",
    "\n",
    "# Instantiate the ESN\n",
    "ESN = esn.EchoStateNetwork(**esn_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning algorithm\n",
    "Choose your learning algo by un-commenting its associated cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Direct methods:**\n",
    "To be used with the following learning algos:\n",
    "- ``RidgeRegression`` (our implementation)\n",
    "- ``RidgeRegression_skl`` (from scikit-learn)\n",
    "- ``LinearSVC`` (from scikit-learn)\n",
    "- ``LogisticRegression_skl`` (from scikit-learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESN.learning_algo = la.RidgeRegression(alpha=10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESN.learning_algo = la.RidgeRegression_skl(alpha=10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESN.learning_algo = la.LinearSVC(C=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESN.learning_algo = la.LogisticRegression_skl()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient descent methods:**\n",
    "To be used with the following learning algos:\n",
    "- ``LogisticRegression`` (our implementation)\n",
    "- ``DeepNN`` (our implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if esn_params['mode'] == 'no_layer':\n",
    "    input_dim = ESN.layer.input_dim\n",
    "else:\n",
    "    input_dim = ESN.layer.dim\n",
    "\n",
    "if esn_params['bidirectional']:\n",
    "    input_dim *= 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ESN.learning_algo = la.LogisticRegression(input_dim=input_dim, output_dim=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESN.learning_algo = la.DeepNN([input_dim, 512, 256, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needs criterion and otpimizer\n",
    "\n",
    "ESN.criterion = torch.nn.CrossEntropyLoss()\n",
    "ESN.optimizer = torch.optim.Adam(ESN.learning_algo.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warm up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put model on device\n",
    "ESN = ESN.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warm up the ESN on multiple sentences\n",
    "if isinstance(ESN.layer, res.LayerRecurrent):\n",
    "    ESN.warm_up(dataset_d['train'].select(range(10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For **direct methods**, the parameters ``epochs`` and ``iter_steps`` are ignored. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing gradient descent...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22ad114e8ce8479295bdcc4f8222b719",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=22.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL STATES torch.Size([1266, 2000])\n",
      "FINAL STATES torch.Size([1536, 2000])\n",
      "FINAL STATES torch.Size([1775, 2000])\n",
      "FINAL STATES torch.Size([1826, 2000])\n",
      "FINAL STATES torch.Size([2048, 2000])\n",
      "FINAL STATES torch.Size([2080, 2000])\n",
      "FINAL STATES torch.Size([2304, 2000])\n",
      "FINAL STATES torch.Size([2304, 2000])\n",
      "FINAL STATES torch.Size([2544, 2000])\n",
      "FINAL STATES torch.Size([2560, 2000])\n",
      "FINAL STATES torch.Size([2752, 2000])\n",
      "FINAL STATES torch.Size([2816, 2000])\n",
      "FINAL STATES torch.Size([3010, 2000])\n",
      "FINAL STATES torch.Size([3072, 2000])\n",
      "FINAL STATES torch.Size([3290, 2000])\n",
      "FINAL STATES torch.Size([3410, 2000])\n",
      "FINAL STATES torch.Size([3630, 2000])\n",
      "FINAL STATES torch.Size([3900, 2000])\n",
      "FINAL STATES torch.Size([4197, 2000])\n",
      "FINAL STATES torch.Size([4621, 2000])\n",
      "FINAL STATES torch.Size([5436, 2000])\n",
      "FINAL STATES torch.Size([2161, 2000])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=22.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL STATES torch.Size([1266, 2000])\n",
      "FINAL STATES torch.Size([1536, 2000])\n",
      "FINAL STATES torch.Size([1775, 2000])\n",
      "FINAL STATES torch.Size([1826, 2000])\n",
      "FINAL STATES torch.Size([2048, 2000])\n",
      "FINAL STATES torch.Size([2080, 2000])\n",
      "FINAL STATES torch.Size([2304, 2000])\n",
      "FINAL STATES torch.Size([2304, 2000])\n",
      "FINAL STATES torch.Size([2544, 2000])\n",
      "FINAL STATES torch.Size([2560, 2000])\n",
      "FINAL STATES torch.Size([2752, 2000])\n",
      "FINAL STATES torch.Size([2816, 2000])\n",
      "FINAL STATES torch.Size([3010, 2000])\n",
      "FINAL STATES torch.Size([3072, 2000])\n",
      "FINAL STATES torch.Size([3290, 2000])\n",
      "FINAL STATES torch.Size([3410, 2000])\n",
      "FINAL STATES torch.Size([3630, 2000])\n",
      "FINAL STATES torch.Size([3900, 2000])\n",
      "FINAL STATES torch.Size([4197, 2000])\n",
      "FINAL STATES torch.Size([4621, 2000])\n",
      "FINAL STATES torch.Size([5436, 2000])\n",
      "FINAL STATES torch.Size([2161, 2000])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=22.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL STATES torch.Size([1266, 2000])\n",
      "FINAL STATES torch.Size([1536, 2000])\n",
      "FINAL STATES torch.Size([1775, 2000])\n",
      "FINAL STATES torch.Size([1826, 2000])\n",
      "FINAL STATES torch.Size([2048, 2000])\n",
      "FINAL STATES torch.Size([2080, 2000])\n",
      "Iteration: 50 Loss: 1.0678523778915405\n",
      "FINAL STATES torch.Size([2304, 2000])\n",
      "FINAL STATES torch.Size([2304, 2000])\n",
      "FINAL STATES torch.Size([2544, 2000])\n",
      "FINAL STATES torch.Size([2560, 2000])\n",
      "FINAL STATES torch.Size([2752, 2000])\n",
      "FINAL STATES torch.Size([2816, 2000])\n",
      "FINAL STATES torch.Size([3010, 2000])\n",
      "FINAL STATES torch.Size([3072, 2000])\n",
      "FINAL STATES torch.Size([3290, 2000])\n",
      "FINAL STATES torch.Size([3410, 2000])\n",
      "FINAL STATES torch.Size([3630, 2000])\n",
      "FINAL STATES torch.Size([3900, 2000])\n",
      "FINAL STATES torch.Size([4197, 2000])\n",
      "FINAL STATES torch.Size([4621, 2000])\n",
      "FINAL STATES torch.Size([5436, 2000])\n",
      "FINAL STATES torch.Size([2161, 2000])\n",
      "\n",
      "\n",
      "Training complete.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0678523778915405]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ESN.fit(dataloader_d[\"train\"], epochs=3, iter_steps=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fc1332e74424431896c69ceb27f5ba4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=22.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL STATES torch.Size([1266, 2000])\n",
      "FINAL STATES torch.Size([1536, 2000])\n",
      "FINAL STATES torch.Size([1775, 2000])\n",
      "FINAL STATES torch.Size([1826, 2000])\n",
      "FINAL STATES torch.Size([2048, 2000])\n",
      "FINAL STATES torch.Size([2080, 2000])\n",
      "FINAL STATES torch.Size([2304, 2000])\n",
      "FINAL STATES torch.Size([2304, 2000])\n",
      "FINAL STATES torch.Size([2544, 2000])\n",
      "FINAL STATES torch.Size([2560, 2000])\n",
      "FINAL STATES torch.Size([2752, 2000])\n",
      "FINAL STATES torch.Size([2816, 2000])\n",
      "FINAL STATES torch.Size([3010, 2000])\n",
      "FINAL STATES torch.Size([3072, 2000])\n",
      "FINAL STATES torch.Size([3290, 2000])\n",
      "FINAL STATES torch.Size([3410, 2000])\n",
      "FINAL STATES torch.Size([3630, 2000])\n",
      "FINAL STATES torch.Size([3900, 2000])\n",
      "FINAL STATES torch.Size([4197, 2000])\n",
      "FINAL STATES torch.Size([4621, 2000])\n",
      "FINAL STATES torch.Size([5436, 2000])\n",
      "FINAL STATES torch.Size([2161, 2000])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train predictions and accuracy\n",
    "train_pred, train_acc = ESN.predict(dataloader_d[\"train\"], verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "387f34387ae54cba82b2b7a75c55aca1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL STATES torch.Size([1582, 2000])\n",
      "FINAL STATES torch.Size([2580, 2000])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test predictions\n",
    "test_pred, test_acc = ESN.predict(dataloader_d[\"test\"], verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.6026    0.7520       229\n",
      "           1     0.4149    0.6964    0.5200        56\n",
      "           2     0.2222    1.0000    0.3636         2\n",
      "           3     0.7231    0.9792    0.8319        48\n",
      "           4     0.8673    0.9423    0.9032       104\n",
      "           5     0.7407    0.9836    0.8451        61\n",
      "\n",
      "    accuracy                         0.7680       500\n",
      "   macro avg     0.6614    0.8674    0.7026       500\n",
      "weighted avg     0.8455    0.7680    0.7750       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test classification report\n",
    "print(classification_report(test_pred.tolist(), \n",
    "                            dataset_d['test']['labels'].tolist(), \n",
    "                            digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 1000, 2000)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ESN.layer.input_dim, ESN.layer.dim, input_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
